{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-09T13:42:23.040019Z","iopub.status.busy":"2022-04-09T13:42:23.039666Z","iopub.status.idle":"2022-04-09T13:42:23.048228Z","shell.execute_reply":"2022-04-09T13:42:23.047033Z","shell.execute_reply.started":"2022-04-09T13:42:23.039983Z"},"papermill":{"duration":0.405206,"end_time":"2022-04-08T14:26:01.787265","exception":false,"start_time":"2022-04-08T14:26:01.382059","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# import maths\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from np.random import randint  \n","\n","import os\n","import csv\n","\n","from glob import glob\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from copy import deepcopy\n","from tqdm import tqdm\n","import random\n","\n","import cv2\n","\n","# pytorch + torchvision\n","import torch \n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter \n","import torch.nn.functional as F\n","\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import torchvision.io as io\n","\n","import kornia\n","\n","# Check that you're using a recent OpenCV version.\n","assert cv2.__version__ > '4.5', 'Please use OpenCV 4.5 or later.'"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007766,"end_time":"2022-04-08T14:26:01.803635","exception":false,"start_time":"2022-04-08T14:26:01.795869","status":"completed"},"tags":[]},"source":["Write down the plan in words and references. We will then fill in code."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T14:26:01.824321Z","iopub.status.busy":"2022-04-08T14:26:01.823683Z","iopub.status.idle":"2022-04-08T14:26:01.82526Z","shell.execute_reply":"2022-04-08T14:26:01.825701Z"},"papermill":{"duration":0.014252,"end_time":"2022-04-08T14:26:01.82586","exception":false,"start_time":"2022-04-08T14:26:01.811608","status":"completed"},"tags":[]},"outputs":[],"source":["# Encoding: ResNeXt (conv 1-3) => Position Enc. (ref. 2107.14222) => Deep-ViT (w/ EfficientNet)\n","# Decoding(for unsupervised training) : HiT(low resolution stage, same # as Deep-ViT) => FCC-GAN\n","# Comparison : Transformer-XL \n","# Classifier/MLP : MLP head (output 8) => Reconstruction module => F\n","\n","# Training Steps:\n","# 1. Train Encoder (unsupervised): manipulate input image (ref. SiT, + rotation) and match to output\n","# 2. Train Comparison (unsupervised) : use different head, mix&match the 2 images (ref. BERT, ALBERT)\n","# 3. Train MLP (supervised) : compare output to F\n","\n","# Inference Steps: Encode each image => concat. 2 images => Comparison => MLP\n","\n","#offical sample code: https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007731,"end_time":"2022-04-08T14:26:01.842112","exception":false,"start_time":"2022-04-08T14:26:01.834381","status":"completed"},"tags":[]},"source":["# Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T13:43:17.302881Z","iopub.status.busy":"2022-04-09T13:43:17.302548Z","iopub.status.idle":"2022-04-09T13:43:17.319887Z","shell.execute_reply":"2022-04-09T13:43:17.319024Z","shell.execute_reply.started":"2022-04-09T13:43:17.302849Z"},"papermill":{"duration":0.02399,"end_time":"2022-04-08T14:26:01.874438","exception":false,"start_time":"2022-04-08T14:26:01.850448","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# copied from sample code\n","# Input data files are available in the read-only \"../input/\" directory.\n","\n","# on kaggle\n","src = '../input/image-matching-challenge-2022/train'\n","\n","# on pc\n","# src = './image-matching-challenge-2022/train'\n","\n","val_scenes = []\n","for f in os.scandir(src):\n","    if f.is_dir():\n","        cur_scene = os.path.split(f)[-1]\n","        print(f'Found scene \"{cur_scene}\"\" at {f.path}')\n","        val_scenes += [cur_scene]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T13:43:21.128167Z","iopub.status.busy":"2022-04-09T13:43:21.127891Z","iopub.status.idle":"2022-04-09T13:43:28.798108Z","shell.execute_reply":"2022-04-09T13:43:28.797216Z","shell.execute_reply.started":"2022-04-09T13:43:21.128138Z"},"papermill":{"duration":7.599256,"end_time":"2022-04-08T14:26:09.482995","exception":false,"start_time":"2022-04-08T14:26:01.883739","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Each scene in the validation set contains a list of images, poses, and pairs. Let's pick one and look at some images.\n","\n","scene = 'piazza_san_marco'\n","\n","images_dict = {}\n","for filename in glob(f'{src}/{scene}/images/*.jpg'):\n","    cur_id = os.path.basename(os.path.splitext(filename)[0])\n","\n","    # OpenCV expects BGR, but the images are encoded in standard RGB, so you need to do color conversion if you use OpenCV for I/O.\n","    images_dict[cur_id] = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n","    \n","print(f'Loaded {len(images_dict)} images.')\n","\n","num_rows = 6\n","num_cols = 4\n","f, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20), constrained_layout=True)\n","for i, key in enumerate(images_dict):\n","    if i >= num_rows * num_cols:\n","        break\n","    cur_ax = axes[i % num_rows, i // num_rows]\n","    cur_ax.imshow(images_dict[key])\n","    cur_ax.set_title(key)\n","    cur_ax.axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# dataset\n","\n","# this loads 2 imgs at once for the first 2 training steps\n","class IMC_images(Dataset):\n","\n","    def __init__(self, landmarks, imgs_file, root_dir, transform=None):\n","        self.landmarks = landmarks\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.buffer_data()\n","\n","    def __len__(self):\n","        return len(self.landmarks_frame)\n","\n","    def __getitem__(self, idx):\n","        landmark1 = random.randint(0, len(self.landmarks))\n","        landmark2 = random.randint(0, len(self.landmarks))\n","\n","        img_num1 = random.randint(0, len(self.images[landmark1]))\n","        img_num1 = random.randint(0, len(self.images[landmark2]))\n","\n","        img1 = self.imgs[landmark1][img_num1]\n","        img2 = self.imgs[landmark2][img_num2]\n","\n","        comp = []\n","        if landmark1 == landmark2 :\n","            comp = [1, 0]\n","        else \n","            comp = [0, 1]\n","        \n","        \n","\n","        sample = {'img1': img1, 'img2': img2, 'comp': comp}\n","\n","        return sample\n","    \n","    def buffer_data(self):\n","        self.imgs = {}\n","        for i, j in enumerate(self.landmarks):\n","            self.imgs[i] = self.transform(\n","                torch_vision.decode_image(file) for file in glob.glob(self.root_dir + j + '/*.jpeg')\n","            ).cpu() # Length * Height * Width\n","\n","\n","# dataset\n","class IMC_dataset(Dataset):\n","    \"\"\"Image Matching Challenge 2022 dataset\"\"\"\n","\n","    def __init__(self, cali_file, covis_file, root_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.calibration = pd.read_csv(cali_file)\n","        self.pair_covisibility = pd.read_csv(covis_file)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.pair_covisibility)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Returns:\n","            image_1 (tensor): First image\n","            image_2 (tensor): Secound image\n","            covisibility (float)\n","            K_1 (matrix): Camera intransic matrix\n","            K_2 (matrix): Camera intransic matrix\n","            R_1 (matrix): Rotation matrix\n","            R_2 (matrix): Rotation matrix\n","            T_1 (vector): Translation vector\n","            T_2 (vector): Translation vector\n","            F (matrix): Fundamental matrix\n","        \"\"\"\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        pairID = self.pair_covisibility[\"pair\"][idx]\n","\n","        img_1_id , img_2_id = pairID.split(\"-\")\n","\n","        F = self.pair_covisibility[\"fundamental_matrix\"][idx]\n","\n","        print(img_1_id, img_2_id) \n","\n","        img_1 = io.read_image(os.path.join(root_dir, img_1_id) + \".jpg\")\n","        img_2 = io.read_image(os.path.join(root_dir, img_2_id) + \".jpg\")\n","\n","        covis = self.pair_covisibility[\"covisibility\"][idx]\n","\n","        idx_1 = self.calibration.loc[self.calibration['image_id'] == img_1_id]\n","        idx_2 = self.calibration.loc[self.calibration['image_id'] == img_2_id]\n","\n","        K_1 = self.calibration['camera_intrinsics'][idx_1]\n","        K_2 = self.calibration['camera_intrinsics'][idx_2]\n","\n","        R_1 = self.calibration['rotation_matrix'][idx_1]\n","        R_2 = self.calibration['rotation_matrix'][idx_2]\n","\n","        T_1 = self.calibration['translation_vector'][idx_1]\n","        T_2 = self.calibration['translation_vector'][idx_2]\n","\n","        sample = {'img_1': img_1, 'img_2': img_2, 'img_2': img_2, 'img_2': img_2, \\\n","         'covisibility': covis, 'K_1': K_1, 'K_2': K_2, 'R_1': R_1, 'R_2': R_2, \\\n","         'T_1': T_1, 'T_2': T_2, \"F\": F}\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample\n","        \n","cali_file = '.\\\\image-matching-challenge-2022\\\\train\\\\brandenburg_gate\\\\calibration.csv'\n","covis_file = \".\\\\image-matching-challenge-2022\\\\train\\\\brandenburg_gate\\\\pair_covisibility.csv\"\n","root_dir = \".\\\\image-matching-challenge-2022\\\\train\\\\brandenburg_gate\\\\images\"\n","\n","dataset = IMC_dataset(cali_file, covis_file, root_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T14:32:59.475677Z","iopub.status.busy":"2022-04-09T14:32:59.475138Z","iopub.status.idle":"2022-04-09T14:32:59.51054Z","shell.execute_reply":"2022-04-09T14:32:59.509448Z","shell.execute_reply.started":"2022-04-09T14:32:59.475646Z"},"papermill":{"duration":0.098892,"end_time":"2022-04-08T14:26:09.68268","exception":false,"start_time":"2022-04-08T14:26:09.583788","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# pytorch data loading\n","def get_scene_trainloader(scene):\n","\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","    trainset = \"./image-matching-challenge-2022/train/\" + scene\n","\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n","                                            shuffle=True, num_workers=2)\n","\n","    classes = ('plane', 'car', 'bird', 'cat',\n","            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","    def imshow(img):\n","        img = img / 2 + 0.5     # unnormalize\n","        npimg = img.numpy()\n","        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","\n","    # get some random training images\n","    dataiter = iter(trainloader)\n","\n","    print(dataiter.next())\n","    images, labels = dataiter.next()\n","\n","    # show images\n","    imshow(torchvision.utils.make_grid(images))\n","    # print labels\n","    print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n","\n","    return trainloader\n","\n","get_scene_trainloader(\"piazza_san_marco\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# image manipulation (ref. SiT)\n","# paper:\n","# github: github.com/Sara-Ahmed/SiT\n","# Copy + Pasted from the github \n","\n","def drop_rand_patches(X, X_rep=None, max_drop=0.3, max_block_sz=0.25, tolr=0.05):\n","    #######################\n","    # X_rep: replace X with patches from X_rep. If X_rep is None, replace the patches with Noise\n","    # max_drop: percentage of image to be dropped\n","    # max_block_sz: percentage of the maximum block to be dropped\n","    # tolr: minimum size of the block in terms of percentage of the image size\n","    #######################\n","    \n","    C, H, W = X.size()\n","    n_drop_pix = np.random.uniform(0, max_drop)*H*W\n","    mx_blk_height = int(H*max_block_sz)\n","    mx_blk_width = int(W*max_block_sz)\n","    \n","    tolr = (int(tolr*H), int(tolr*W))\n","    \n","    total_pix = 0\n","    while total_pix < n_drop_pix:\n","        \n","        # get a random block by selecting a random row, column, width, height\n","        rnd_r = randint(0, H-tolr[0])\n","        rnd_c = randint(0, W-tolr[1])\n","        rnd_h = min(randint(tolr[0], mx_blk_height)+rnd_r, H) #rnd_r is alread added - this is not height anymore\n","        rnd_w = min(randint(tolr[1], mx_blk_width)+rnd_c, W)\n","        \n","        if X_rep is None:\n","            X[:, rnd_r:rnd_h, rnd_c:rnd_w] = torch.empty((C, rnd_h-rnd_r, rnd_w-rnd_c), dtype=X.dtype, device='cuda').normal_()\n","        else:\n","            X[:, rnd_r:rnd_h, rnd_c:rnd_w] = X_rep[:, rnd_r:rnd_h, rnd_c:rnd_w]    \n","         \n","        total_pix = total_pix + (rnd_h-rnd_r)*(rnd_w-rnd_c)\n","\n","    return X\n","\n","def rgb2gray_patch(X, tolr=0.05):\n","\n","    C, H, W = X.size()\n","    tolr = (int(tolr*H), int(tolr*W))\n","     \n","    # get a random block by selecting a random row, column, width, height\n","    rnd_r = randint(0, H-tolr[0])\n","    rnd_c = randint(0, W-tolr[1])\n","    rnd_h = min(randint(tolr[0], H)+rnd_r, H) #rnd_r is alread added - this is not height anymore\n","    rnd_w = min(randint(tolr[1], W)+rnd_c, W)\n","    \n","    X[:, rnd_r:rnd_h, rnd_c:rnd_w] = torch.mean(X[:, rnd_r:rnd_h, rnd_c:rnd_w], dim=0).unsqueeze(0).repeat(C, 1, 1)\n","\n","    return X\n","    \n","\n","def smooth_patch(X, max_kernSz=15, gauss=5, tolr=0.05):\n","\n","    #get a random kernel size (odd number)\n","    kernSz = 2*(randint(3, max_kernSz+1)//2)+1\n","    gausFct = np.random.rand()*gauss + 0.1 # generate a real number between 0.1 and gauss+0.1\n","    \n","    C, H, W = X.size()\n","    tolr = (int(tolr*H), int(tolr*W))\n","     \n","    # get a random block by selecting a random row, column, width, height\n","    rnd_r = randint(0, H-tolr[0])\n","    rnd_c = randint(0, W-tolr[1])\n","    rnd_h = min(randint(tolr[0], H)+rnd_r, H) #rnd_r is alread added - this is not height anymore\n","    rnd_w = min(randint(tolr[1], W)+rnd_c, W)\n","    \n","    \n","    gauss = kornia.filters.GaussianBlur2d((kernSz, kernSz), (gausFct, gausFct))\n","    X[:, rnd_r:rnd_h, rnd_c:rnd_w] = gauss(X[:, rnd_r:rnd_h, rnd_c:rnd_w].unsqueeze(0))\n","    \n","    return X\n","\n","def random_rotation(X, orientation=None):\n","    # generate random number between 0 and n_rot to represent the rotation\n","    if orientation is None:\n","        orientation = np.random.randint(0, 4)\n","    \n","    if orientation == 0: # do nothing\n","        pass\n","    elif orientation == 1:  \n","        X = X.rot90(-1, [1, 2])\n","    elif orientation == 2:  \n","        X = X.rot90(-1, [1, 2]).rot90(-1, [1, 2])\n","    elif orientation == 3: \n","        X = X.rot90(1, [1, 2])\n","        \n","    return X, orientation\n","\n","def distortImages(samples):\n","    n_imgs = samples.size()[0] #this is batch size, but in case bad inistance happened while loading\n","    samples_aug = samples.detach().clone()\n","    for i in range(n_imgs):\n","        samples_aug[i] = random_rotation(samples_aug[i])\n","        \n","        samples_aug[i] = rgb2gray_patch(samples_aug[i])\n","\n","        samples_aug[i] = smooth_patch(samples_aug[i])\n","\n","        samples_aug[i] = drop_rand_patches(samples_aug[i])\n","\n","        idx_rnd = randint(0, n_imgs)\n","        if idx_rnd != i:\n","            samples_aug[i] = drop_rand_patches(samples_aug[i], samples_aug[idx_rnd])\n","      \n","    return samples_aug   "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# loss \n","from pytorch_wavelets import DWTInverse, DWTForward\n","\n","class DistillerLoss(nn.Module):\n","    def __init__(\n","            self,\n","            loss_weights={\"l1\": 1.0, \"l2\": 1.0, \"loss_p\": 1.0, \"loss_g\": 0.5}\n","    ):\n","        super().__init__()\n","        # l1/l2 loss\n","        self.l1_loss = nn.L1Loss()\n","        self.l2_loss = nn.MSELoss()\n","        \n","        # loss weights\n","        self.loss_weights = loss_weights\n","        # utils\n","        self.dwt = DWTForward(J=1, mode='zero', wave='db1')\n","        self.idwt = DWTInverse(mode=\"zero\", wave=\"db1\")\n","\n","    def forward(self, pred, gt):\n","        # l1/l2 loss\n","        loss = {\"l1\": 0, \"l2\": 0}\n","        for _pred in pred[\"freq\"]:\n","            _pred_rgb = self.dwt_to_img(_pred)\n","            _gt_rgb = F.interpolate(gt[\"img\"], size=_pred_rgb.size(-1), mode='bilinear', align_corners=True)\n","            _gt_freq = self.img_to_dwt(_gt_rgb)\n","            loss[\"l1\"] += self.l1_loss(_pred_rgb, _gt_rgb)\n","            loss[\"l2\"] += self.l2_loss(_pred_rgb, _gt_rgb)\n","            loss[\"l1\"] += self.l1_loss(_pred, _gt_freq)\n","            loss[\"l2\"] += self.l2_loss(_pred, _gt_freq)\n","            \n","        # total loss\n","        loss[\"loss\"] = 0\n","        for k, w in self.loss_weights.items():\n","            if loss[k] is not None:\n","                loss[\"loss\"] += w * loss[k]\n","            else:\n","                del loss[k]\n","        return loss[\"loss\"]\n","\n","    def img_to_dwt(self, img):\n","        low, high = self.dwt(img)\n","        b, _, _, h, w = high[0].size()\n","        high = high[0].view(b, -1, h, w)\n","        freq = torch.cat([low, high], dim=1)\n","        return freq\n","\n","    def dwt_to_img(self, img):\n","        b, c, h, w = img.size()\n","        low = img[:, :3, :, :]\n","        high = img[:, 3:, :, :].view(b, 3, 3, h, w)\n","        return self.idwt((low, [high]))\n","\n","class ComparisonLoss(nn.Module): #work in progress,  but binary cross entropy loss for now\n","    def __init__(self):\n","        super().__init__()\n","        self.bcel = nn.BCELoss()\n","    \n","    def forward(self, pred, gt):\n","        return = self.bcel(pred, gt)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch_xla.core.xla_model as xm\n","import torch_xla.distibuted.parallel_loader as pl\n","\n","# training loop \n","def train_SSL(\n","        encoder, decoder, comparison, \n","        recon_loss, comp_loss, loss_crit,\n","        train_loader: Iterable, optimizer: torch.optim.Optimizer,\n","        device: torch.device, writer, save_loc\n","        epoch: int, max_norm: float = 0, mixup_fn: Optional[Mixup] = None\n","    ):\n","    model.train(True)\n","    header = 'Epoch: [{}]'.format(epoch)\n","    print_freq = 50\n","\n","    loader = pl.MpDeviceLoader(train_loader, device)\n","\n","    #training \n","    for i, train_data in enumerate(loader):\n","        imgs1_aug = distortImages(train_data[\"img1\"]) # Apply distortion\n","        imgs2_aug = distortImages(train_data[\"img2\"]) \n","\n","        \n","        optimizer.zero_grad()\n","\n","        with torch.autocast():\n","            img1_enc = encoder(img1_aug) \n","            img1_recon = decoder(img1_enc)\n","\n","            img2_enc2 = encoder(img2_aug) \n","            img2_recon = decoder(img2_enc)\n","\n","            imgs_comp = comparison(torch.cat([img1_enc, img2_enc], dim=0))\n","\n","            imgs_recon = torch.cat([imgs1_recon, imgs2_recon], dim=0) \n","            imgs = torch.cat([imgs1, imgs2], dim=0) \n","\n","            loss = recon_loss(imgs_recon, imgs) + comp_loss(imgs_comp1, train_data[\"comp\"])\n","            loss.backward()\n","        \n","            xm.optimizer_step(optimizer)\n","\n","        writer.add_scalar(\"training loss\", loss, i * imgs.size()[0])\n","\n","        if loss < loss_crit :\n","            torch.save(\n","                {\n","                    \"enc\" : encoder.state_dict()\n","                    \"dec\" : decoder.state_dict(),\n","                    \"comp\" : comparison.state_dict()\n","                    \"loss\" : loss.state_dict()\n","                },\n","                save_loc\n","            )\n","            \n","            break \n","\n","\n","    #evaluating\n","    #commented out because its not very useful\n","    '''loader = pl.MpDeviceLoader(test_loader, device)\n","\n","    for i, img1, img2, same in enumerate(loader):\n","        imgs_aug = distortImages(imgs1) # Apply distortion\n","\n","        with torch.autocast():\n","            imgs_recon = model(imgs_aug) \n","\n","            loss.append(loss(imgs_recon, imgs))\n","\n","        writer.add_scalar(\"testing loss\", loss, i * imgs.size()[0])'''"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T14:26:09.885364Z","iopub.status.busy":"2022-04-08T14:26:09.884364Z","iopub.status.idle":"2022-04-08T14:26:09.8879Z","shell.execute_reply":"2022-04-08T14:26:09.888436Z"},"papermill":{"duration":0.10691,"end_time":"2022-04-08T14:26:09.888627","exception":false,"start_time":"2022-04-08T14:26:09.781717","status":"completed"},"tags":[]},"outputs":[],"source":["# Summary: A combination of ResNeXt and Squeeze and Excitation Network (SENet) to \n","# ResNeXt ref: https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n","# SENet ref: https://github.com/moskomule/senet.pytorch \n","\n","def conv1x1(in_channels, out_channels, stride = 1):\n","    \"\"\" 1x1 convolution\"\"\"\n","    return nn.Conv2d(\n","        in_channels, out_channels,\n","        kernel_size = 1, \n","        stride = stride,\n","        bias = False\n","    )\n","\n","def conv3x3(in_channels, out_channels, stride = 1, groups = 1, dilation = 1):\n","    \"\"\" 3x3 convolution \"\"\"\n","    return nn.Conv2d(\n","        in_channels, out_channels,\n","        kernel_size = 3,\n","        stride = stride,\n","        padding = dilation,\n","        groups = groups,\n","        bias = False,\n","        dilation = dilation \n","    )\n","\n","def conv5x5(in_channels, out_channels, stride = 1, groups = 1, dilation = 1):\n","    return nn.Conv2d(\n","        in_channels, out_channels,\n","        kernel_size = 5,\n","        stride = stride,\n","        padding = dilation,\n","        groups = groups,\n","        bias = False,\n","        dilation = dilation \n","    )\n","\n","class BottleNeck(nn.Module):\n","    \"\"\" BottleNeck Layer in ResNet \"\"\"\n","\n","    expansion : int = 4 \n","\n","    def __init__(\n","        self, in_channels, out_channels, \n","        reduction=2, stride=1, downsample=None, num_groups=64,\n","        conv=conv3x3\n","    ):\n","        # if we want to use the expansion factor, we can just modify \"out_channels\"\n","        # out_channels = in_channels * self.expansion\n","\n","        super().__init__()\n","\n","        width = int(in_channels/reduction)\n","\n","        # inplace is used for ReLU to reduce memory usage\n","        self.resnext_block = nn.Sequential(\n","            conv1x1(in_channels, width),\n","            nn.BatchNorm2d(width),\n","            nn.ReLU(inplace=True),\n","\n","            conv(width, width, stride=stride, groups=num_groups),\n","            nn.BatchNorm2d(width),\n","            nn.ReLU(inplace=True),\n","\n","            conv1x1(width, out_channels),\n","            norm_layer(out_channels)\n","        )\n","\n","        if downsample is not None:\n","            self.downsample = downsample\n","\n","        self.activation = nn.ReLU(inplace=True)\n","    \n","    def forward(self, x):\n","        residual = x \n","        out = self.resnext_block(x)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","            \n","        out += residual\n","        out = self.activation(out)\n","        return out\n","\n","class SELayer(nn.Module):\n","    \"\"\" building block described in the SENet Paper and github\"\"\"\n","    def __init__(self, channel, reduction=16):\n","        super().__init__())\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Sequential(\n","            nn.Linear(channel, in_channel//reduction, bias=False),\n","            nn.ReLU(inplace=True)\n","            nn.Linear(in_channel//reduction, channel, bias=False),\n","            nn.Sigmoid()\n","        )\n","    \n","    def forward(self, x):\n","        batch, channel, _, _ = x.size()\n","        y = self.avg_pool(x).view(batch, channel)\n","        y = self.fc(y).view(batch, channel, 1, 1)\n","        return x * y.expand_as(x)\n","\n","class ResNeXtLayer(nn.Module):\n","    \"\"\" basic layer described in the ResNeXt Paper \"\"\"\n","    def __init__(self, in_channels, out_channels, num_blocks, num_groups=64, dilation=1, stride=1):\n","        super().__init__()\n","        downsample = nn.Sequential(\n","            conv1x1(in_channels, out_channels, stride),\n","            nn.BatchNorm2d(out_channels)\n","        )\n","\n","        layers = []\n","        layers.append(BottleNeck(in_channels, out_channels, downsample=downsample, stride=stride))\n","        \n","        for _ in range(1, num_blocks):\n","            layers += [\n","                BottleNeck(out_channels, out_channels, dilation=dilation, num_groups=num_groups),\n","                SELayer(out_channels)\n","            ]\n","        \n","        self.layers = nn.Sequential(*layers)\n","\n","        \n","    def forward(self, x):\n","        x = self.layers(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CAPE: Continuous Augmented Positional Embeddings\n","# paper @ https://arxiv.org/2106.03143\n","\n","class CAPE(nn.Module): # I have offically given up on encoding in general\n","    def __init__(\n","        self, model_dim \n","        max_global_shift = 0.0, max_local_shift = 0.0, max_global_scaling = 1.0, \n","    ):\n","        self.max_global_shift = max_global_shift\n","        self.max_local_shift = max_local_shift\n","        self.max_global_scaling = max_global_scaling\n","\n","        self.register_buffer('content_scale', nn.Tensor([math.sqrt(model_dim)]))\n","\n","    def forward(self, patches):\n","        return (patches * self.content_scale) + self.compute_pos_emb(patches)\n","    \n","    def compute_pos_emb(self, patches):\n","        batch, height, width, channel = patches.shape()\n","\n","        x = torch.zeros\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TransCNN + Universal Transformer \n","# Idea : each layer repeated via ACT, then down sampled\n","# Idea : direct downsampling for residual\n","\n","class CNNAttention(nn.Module):\n","    def __init__(self, total_dim, head_dim, conv=conv1x1, grid_size=1, downsample_rate=1, drop=0):\n","        super().__init__()\n","        self.num_heads = total_dim // head_dim # area of previous step / area of head\n","        self.head_dim = head_dim \n","        self.side_len = self.head_dim ** -0.5\n","        self.grid_size = grid_size\n","\n","        self.norm = nn.BatchNorm2d()\n","        self.qkv = conv(total_dim, total_dim * 3)\n","        self.proj = nn.Conv2d(total_dim, total_dim)\n","        self.drop = nn.Dropout2d(drop, inplace=True)\n","\n","        if self.grid_size > 1:\n","            self.q = conv(total_dim, total_dim)\n","            self.kv = conv(total_dim, total_dim * 2)\n","\n","            self.grid_norm = nn.BatchNorm2d(total_dim)\n","            self.avg_pool = nn.AvgPool2d(total_dim)\n","            self.downsample_norm = nn.BatchNorm2d(total_dim)\n","\n","    def forward(self, x):\n","        batch, channels, height, width = x.shape()\n","        qkv = self.qkv(self.norm(x))\n","\n","        if self.grid_size > 1:\n","            # compute grid based/local attention\n","            grid_h, grid_w = height // self.grid_size, width // self.grid_size # H/G, W/G\n","            qkv = qkv.reshape(\n","                batch, 3, # q, k, v\n","                self.num_heads, self.head_dim, \n","                grid_h, self.grid_size,\n","                grid_w, self.grid_size\n","            ) # ref. the dimensions of this space is R^ Batch * QKV * Head * Size * H/G * G * W/G * G\n","            qkv = qkv.permute(1, 0, 2, 4, 6, 5, 7, 3) # R^ QKV * Batch * Head * H/G * W/G * G * G * Size \n","            qkv = qkv.reshape(3, -1, self.grid_size ** 2, self.head_dim) # R^ QKV * (Batch * Head * H/ * G * G * Size \n","            q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","            attn = (q @ k.transpose(-2, -1)) * self.side_len # transpose k -> R^ Batch * G * G * (W * H)\n","            attn = attn.softmax(dim=-1)\n","            grid_x = (attn @ v).reshape(\n","                batch, self.num_heads, \n","                grid_h, grid_w, \n","                self.grid_size, self.grid_size, \n","                self.head_dim \n","            ) # R^ Batch * Head * H/G * W/G * G * G * Size, same as after permute\n","            grid_x = self.grid_norm(x + grid_x) #residue and normalisation\n","\n","            # transform qkv for computing global attention\n","            q = self.q(grid_x).reshape(batch, self.num_heads, self.head_dim, -1) # R^ Batch * Head * Size * (H * W)\n","            q = q.transpose(-2, -1) # R^ Batch * Head * (H * W) * Size \n","            kv = self.kv(self.downsample_norm(self.avg_pool(grid_x)))\n","            kv = kv.reshape(batch, 2, self.num_heads, self.head_dim, -1) # R^ Batch * KV * Head * Size * (H * W)\n","            kv = kv.permute(1, 0, 2, 4, 3) # R^ KV * Batch * Head *  (H * W) * Size \n","            k, v = kv[0], kv[1] # R^ Batch * Head * (H * W) * Size\n","        else: \n","            # transform qkv for computing global attention\n","            qkv = qkv.reshape(B, 3, self.num_heads, self.head_dim, -1) # R^ Batch * QKV * Head * Size * (H * W)\n","            qkv = qkv.permute(1, 0, 2, 4, 3) # R^ QKV * Batch * Head * (H * W) * Size\n","            q, k, v = qkv[0], qkv[1], qkv[2]\n","        \n","        # compute global attention\n","        attn = (q @ k.transpose(-2, -1)) * self.side_len \n","        attn = attn.softmax(dim=1)\n","        global_x = (attn @ v).transpose(-2, -1).reshape(batch, channel, height, width)\n","\n","        # residue\n","        if self.grid_size > 1\n","            global_x += grid_x\n","        x = self.drop(self.proj(global_x))\n","\n","        return x \n","\n","class ACT(nn.Module):\n","\n","    threshold = 1 - 0.1\n","\n","    def __init__(self, module, size, max_steps, activation=nn.Sigmoid):\n","        super().__init__()\n","        self.module = module\n","        self.activation = activation()\n","        self.fc = nn.Linear(size, 1) # What if we replace linear with conv2d?\n","\n","        self.max_steps = max_steps\n","        # !!!be sure to initialise self.p!!!\n","    \n","    def forward(self, state):\n","        # change the shape of input to 3d for ACT\n","        shape = state.size()\n","        if len(shape) > 3\n","            state = state.view((*shape[:1], )) \n","        batch, size, _ = state.size()\n","\n","        halting_probablity = torch.zeros(batch, size)\n","        remainders = torch.zeros(batch, size)\n","        n_updates = torch.zeros(batch, size)\n","        previous_state = torch.zeros_like(inputs)\n","\n","        def should_continue(h, n, m):\n","            return (h < self.threshold and n < m).byte().any()\n","        while should_continue(halting_probability, n_updates, self.max_steps):\n","            # we are avoiding timing signals because we have our own RPE \n","            state = self.activation(self.fc(state)).squeeze(-1) \n","\n","            # calculate masks for which ones to halt\n","            still_running = (halting_probability < 1.0).float()\n","            new_halted = (halting_probability + state * still_running > self.threshold).float() * still_running\n","            still_running = (halting_probability + state * still_running <= self.threshold).float() * still_running\n","\n","            # halt parameters and increment remainders\n","            halting_probability += state * still_running\n","            remainders += new_halted * (1 - halting_probability)\n","            halting_probability += new_halted * remainders \n","            n_updates += still_running + new_halted\n","            # compute weights to apply to the state and output\n","            update_weights = state * still_running + new_halted * remainders \n","            \n","            state = self.module(state)\n","            previous_state = state * update_weights.unsqueeze(-1) + previous_state * (1 - update_weights.unsqueeze(-1))\n","\n","            step += 1\n","            \n","        # change the shape back to the original\n","        previous_state = previous_state.view((*shape, ))\n","        return previous_state, (remainders, n_updates)\n","\n","class TransDownsample(nn.Module):\n","    def __init__(self, in_dim, out_dim, activation=nn.SiLU):\n","        super().__init__()\n","        self.conv = conv3x3(in_dim, out_dim, stride=2)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.residual = nn.Conv2d(in_dim, out_dim, 1)\n","        self.norm1 = nn.BatchNorm2d(out_dim)\n","        self.norm2 = nn.BatchNorm2d(out_dim)\n","        self.act = activation(inplace=True)\n","\n","    def forward(self, x):\n","        x1 = self.norm1(self.conv(x))\n","        x2 = self.norm2(self.residual(self.pool(x)))\n","        x = self.act(x1 + x2)\n","        return x\n","\n","class TransCNNLayer(nn.Module):\n","    def __init__(\n","        self, in_dim, out_dim, depth, max_steps, \n","        head_dim=64, conv=conv1x1, grid_size=1, downsample_rate=1, drop=0,\n","        bnconv=conv3x3\n","    ):\n","        super().__init__()\n","\n","        self.down_sample = TransDownsample(in_dim, out_dim)\n","\n","        trans_cnn = []\n","        for i in range(depth):\n","            transcnn_block = nn.Sequential(\n","                CNNAttention(\n","                    out_dim,\n","                    head_dim,\n","                    conv=conv,\n","                    grid_size=grid_size,\n","                    downsample_rate=downsample_rate,\n","                    drop=drop\n","                ),\n","                BottleNeck(total_dim, total_dim, conv=bnconv)\n","            )\n","            trans_cnn.append(ACT(transcnn_block, total_dim, max_steps))\n","        self.trans_cnn = nn.ModuleList(*seq)\n","        \n","    \n","    def forward(self, x):\n","        for trans_cnn in self.trans_cnn:\n","            x, _ = trans_cnn(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# MobileStyleGAN w/o the style\n","# Q: Can we just ctrl c+p this?\n","# A: Yes! Yes we can!\n","\n","from pytorch_wavelets.dwt.lowlevel import *\n","\n","def _SFB2D(low, highs, g0_row, g1_row, g0_col, g1_col, mode):\n","    mode = int_to_mode(mode)\n","\n","    lh, hl, hh = torch.unbind(highs, dim=2)\n","    lo = sfb1d(low, lh, g0_col, g1_col, mode=mode, dim=2)\n","    hi = sfb1d(hl, hh, g0_col, g1_col, mode=mode, dim=2)\n","    y = sfb1d(lo, hi, g0_row, g1_row, mode=mode, dim=3)\n","\n","    return y\n","\n","class DWTInverse(nn.Module):\n","    \"\"\" Performs a 2d DWT Inverse reconstruction of an image\n","    Args:\n","        wave (str or pywt.Wavelet): Which wavelet to use\n","        C: deprecated, will be removed in future\n","    \"\"\"\n","    def __init__(self, wave='db1', mode='zero', trace_model=False):\n","        super().__init__()\n","        if isinstance(wave, str):\n","            wave = pywt.Wavelet(wave)\n","        if isinstance(wave, pywt.Wavelet):\n","            g0_col, g1_col = wave.rec_lo, wave.rec_hi\n","            g0_row, g1_row = g0_col, g1_col\n","        else:\n","            if len(wave) == 2:\n","                g0_col, g1_col = wave[0], wave[1]\n","                g0_row, g1_row = g0_col, g1_col\n","            elif len(wave) == 4:\n","                g0_col, g1_col = wave[0], wave[1]\n","                g0_row, g1_row = wave[2], wave[3]\n","        # Prepare the filters\n","        filts = prep_filt_sfb2d(g0_col, g1_col, g0_row, g1_row)\n","        self.register_buffer('g0_col', filts[0])\n","        self.register_buffer('g1_col', filts[1])\n","        self.register_buffer('g0_row', filts[2])\n","        self.register_buffer('g1_row', filts[3])\n","        self.mode = mode\n","        self.trace_model = trace_model\n","\n","    def forward(self, coeffs):\n","        yl, yh = coeffs\n","        ll = yl\n","        mode = mode_to_int(self.mode)\n","\n","        for h in yh[::-1]:\n","            if h is None:\n","                h = torch.zeros(ll.shape[0], ll.shape[1], 3, ll.shape[-2],\n","                                ll.shape[-1], device=ll.device)\n","\n","            if ll.shape[-2] > h.shape[-2]:\n","                ll = ll[...,:-1,:]\n","            if ll.shape[-1] > h.shape[-1]:\n","                ll = ll[...,:-1]\n","            if not self.trace_model:\n","                ll = SFB2D.apply(ll, h, self.g0_col, self.g1_col, self.g0_row, self.g1_row, mode)\n","            else:\n","                ll = _SFB2D(ll, h, self.g0_col, self.g1_col, self.g0_row, self.g1_row, mode)\n","        return ll\n","\n","class IDWTUpsample(nn.Module):\n","    def __init__(\n","            self,\n","            channels_in,\n","    ):\n","        super().__init__()\n","        self.channels = channels_in // 4\n","        assert self.channels * 4 == channels_in\n","        # upsample\n","        self.idwt = DWTInverse(mode='zero', wave='db1')\n","\n","    def forward(self, x):\n","        b, _, h, w = x.size()\n","        low = x[:, :self.channels]\n","        high = x[:, self.channels:]\n","        high = high.view(b, self.channels, 3, h, w)\n","        x = self.idwt((low, [high]))\n","        return x\n","\n","class ModulatedConv2d(nn.Module):\n","    def __init__(\n","            self,\n","            channels_in,\n","            channels_out,\n","            kernel_size,\n","    ):\n","        super().__init__()\n","        # create conv\n","        self.weight = nn.Parameter(\n","            torch.randn(channels_out, channels_in, kernel_size, kernel_size)\n","        )\n","        # some service staff\n","        self.scale = 1.0 / math.sqrt(channels_in * kernel_size ** 2)\n","        self.padding = kernel_size // 2\n","\n","    def forward(self, x):\n","        x = F.conv2d(x, self.weight, padding=self.padding)\n","        return x\n","\n","\n","class ModulatedDWConv2d(nn.Module):\n","    def __init__(\n","            self,\n","            channels_in,\n","            channels_out,\n","            kernel_size,\n","    ):\n","        super().__init__()\n","        # create conv\n","        self.weight_dw = nn.Parameter(\n","            torch.randn(channels_in, 1, kernel_size, kernel_size)\n","        )\n","        self.weight_permute = nn.Parameter(\n","            torch.randn(channels_out, channels_in, 1, 1)\n","        )\n","        # some service staff\n","        self.scale = 1.0 / math.sqrt(channels_in * kernel_size ** 2)\n","        self.padding = kernel_size // 2\n","\n","    def forward(self, x):\n","        x = F.conv2d(x, self.weight_dw, padding=self.padding, groups=x.size(1))\n","        x = F.conv2d(x, self.weight_permute)\n","        return x\n","\n","class StyledConv2d(nn.Module):\n","    def __init__(\n","        self,\n","        channels_in,\n","        channels_out,\n","        kernel_size,\n","        conv_module\n","    ):\n","        super().__init__()\n","\n","        self.conv = conv_module(\n","            channels_in,\n","            channels_out,\n","            kernel_size,\n","        )\n","\n","        self.bias = nn.Parameter(torch.zeros(1, channels_out, 1, 1))\n","        self.act = nn.LeakyReLU(0.2)\n","\n","    def forward(self, input):\n","        out = self.conv(input)\n","        out = self.act(out + self.bias)\n","        return out\n","\n","class MultichannelImage(nn.Module):\n","    def __init__(\n","            self,\n","            channels_in,\n","            channels_out,\n","            kernel_size=1\n","    ):\n","        super().__init__()\n","        self.conv = ModulatedConv2d(channels_in, channels_out, kernel_size, demodulate=False)\n","        self.bias = nn.Parameter(torch.zeros(1, channels_out, 1, 1))\n","\n","    def forward(self, hidden):\n","        out = self.conv(hidden)\n","        out = out + self.bias\n","        return out\n","\n","class MobileSynthesisBlock(nn.Module):\n","    def __init__(\n","            self,\n","            channels_in,\n","            channels_out,\n","            kernel_size=3,\n","            conv_module\n","    ):\n","        super().__init__()\n","        self.up = IDWTUpsample(channels_in)\n","        self.conv1 = StyledConv2d(\n","            channels_in // 4,\n","            channels_out,\n","            kernel_size,\n","            conv_module=conv_module\n","        )\n","        self.conv2 = StyledConv2d(\n","            channels_out,\n","            channels_out,\n","            kernel_size,\n","            conv_module=conv_module\n","        )\n","        self.to_img = MultichannelImage(\n","            channels_in=channels_out,\n","            channels_out=12,\n","            kernel_size=1\n","        )\n","\n","    def forward(self, hidden):\n","        hidden = self.up(hidden)\n","        hidden = self.conv1(hidden)\n","        hidden = self.conv2(hidden)\n","        img = self.to_img(hidden)\n","        return hidden, img\n","\n","    def wsize(self):\n","        return 3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, height, width, head_dim=64, num_channels=3):\n","        # conv7x7 + ResNet [1-3] + TransCNN [4-5]\n","        super().__init__()\n","        self.seq = nn.Sequential(\n","            nn.Conv2d(num_channels, 64, 7, stride=2),\n","            nn.MaxPool2d(3, stride=2),\n","            \n","            ResNextLayer(64, 256, 3, num_groups=32, stride=2),\n","            ResNextLayer(256, 512, 4, num_groups=32, stride=2),\n","        \n","            TransCNNLayer(512, 256, 4, 8, bnconv=conv5x5, head_dim=head_dim),\n","            TransCNNLayer(256, 512, 2, 4, bnconv=conv3x3, head_dim=head_dim)\n","        )\n","    \n","    def forward(self, x):\n","        x = self.seq(x)\n","        return x\n","\n","class Decoder(nn.Module):\n","    def __init__(\n","            self, head_dim=64, conv=conv1x1, grid_size=1,\n","            trans_channels = [512, 512, 512],\n","            cnn_channels = [512, 512, 256, 128, 64]\n","    ):\n","        super().__init__()\n","\n","        # We need to add \n","        # Backwards version of TransCNN\n","        self.trans_layers = nn.ModuleList()\n","        for i, channels_out in enumerate(trans_channels):\n","            transcnn_block = CNNAttention(\n","                channels_out,\n","                head_dim,\n","                conv=conv,\n","                grid_size=grid_size,\n","                downsample_rate=1,\n","                drop=0\n","            )\n","            trans_layers.append(ACT(transcnn_block, total_dim, max_steps))\n","\n","        self.cnn_layers = nn.ModuleList()\n","        channels_in = cnn_channels[0]\n","        for i, channels_out in enumerate(cnn_channels[1:]):\n","            self.cnn_layers.append(\n","                MobileSynthesisBlock(\n","                    channels_in,\n","                    channels_out,\n","                    3,\n","                    conv_module=ModulatedDWConv2d\n","                )\n","            )\n","            channels_in = channels_out\n","\n","        self.idwt = DWTInverse(mode=\"zero\", wave=\"db1\")\n","        self.register_buffer(\"device_info\", torch.zeros(1))\n","        self.trace_model = False\n","\n","    def forward(self, x):\n","        out = {\"freq\": [], \"img\": None}\n","\n","        for trans_cnn in self.trans_layers:\n","            x, _ = trans_cnn(x)\n","        \n","        for i, m in enumerate(self.cnn_layers):\n","            x, freq = m(x)\n","            out[\"freq\"].append(freq)\n","\n","        out[\"img\"] = self.dwt_to_img(out[\"freq\"][-1])\n","        return out\n","\n","    def dwt_to_img(self, img):\n","        b, c, h, w = img.size()\n","        low = img[:, :3, :, :]\n","        high = img[:, 3:, :, :].view(b, 3, 3, h, w)\n","        return self.idwt((low, [high]))\n","\n","    def wsize(self):\n","        return len(self.cnn_layers) * self.cnn_layers[0].wsize() + 2\n","\n","class CompHead(nn.Module):\n","    def __init__(self, in_size, comp_size):\n","        super().__init__()\n","        self.seq = nn.Sequential(\n","            nn.Linear(in_size, in_size),\n","            nn.Linear(in_size, comp_size)\n","        )\n","    \n","    def forward(self, x):\n","        x = x.flatten()\n","        x = self.seq(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
