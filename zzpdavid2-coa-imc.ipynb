{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-09T13:42:23.040019Z","iopub.status.busy":"2022-04-09T13:42:23.039666Z","iopub.status.idle":"2022-04-09T13:42:23.048228Z","shell.execute_reply":"2022-04-09T13:42:23.047033Z","shell.execute_reply.started":"2022-04-09T13:42:23.039983Z"},"papermill":{"duration":0.405206,"end_time":"2022-04-08T14:26:01.787265","exception":false,"start_time":"2022-04-08T14:26:01.382059","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# imports\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","import csv\n","\n","from glob import glob\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from copy import deepcopy\n","from tqdm import tqdm\n","import random\n","\n","import cv2\n","\n","# pytorch + torchvision\n","import torch \n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","\n","# Check that you're using a recent OpenCV version.\n","assert cv2.__version__ > '4.5', 'Please use OpenCV 4.5 or later.'"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007766,"end_time":"2022-04-08T14:26:01.803635","exception":false,"start_time":"2022-04-08T14:26:01.795869","status":"completed"},"tags":[]},"source":["Write down the plan in words and references. We will then fill in code."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T14:26:01.824321Z","iopub.status.busy":"2022-04-08T14:26:01.823683Z","iopub.status.idle":"2022-04-08T14:26:01.82526Z","shell.execute_reply":"2022-04-08T14:26:01.825701Z"},"papermill":{"duration":0.014252,"end_time":"2022-04-08T14:26:01.82586","exception":false,"start_time":"2022-04-08T14:26:01.811608","status":"completed"},"tags":[]},"outputs":[],"source":["# Encoding: ResNeXt (conv 1-3) => Position Enc. (ref. 2107.14222) => Deep-ViT (w/ EfficientNet)\n","# Decoding(for unsupervised training) : HiT(low resolution stage, same # as Deep-ViT) => FCC-GAN\n","# Comparison : Transformer-XL \n","# Classifier/MLP : MLP head (output 8) => Reconstruction module => F\n","\n","# Training Steps:\n","# 1. Train Encoder (unsupervised): manipulate input image (ref. SiT, + rotation) and match to output\n","# 2. Train Comparison (unsupervised) : use different head, mix&match the 2 images (ref. BERT, ALBERT)\n","# 3. Train MLP (supervised) : compare output to F\n","\n","# Inference Steps: Encode each image => concat. 2 images => Comparison => MLP\n","\n","#offical sample code: https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007731,"end_time":"2022-04-08T14:26:01.842112","exception":false,"start_time":"2022-04-08T14:26:01.834381","status":"completed"},"tags":[]},"source":["# Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T13:43:17.302881Z","iopub.status.busy":"2022-04-09T13:43:17.302548Z","iopub.status.idle":"2022-04-09T13:43:17.319887Z","shell.execute_reply":"2022-04-09T13:43:17.319024Z","shell.execute_reply.started":"2022-04-09T13:43:17.302849Z"},"papermill":{"duration":0.02399,"end_time":"2022-04-08T14:26:01.874438","exception":false,"start_time":"2022-04-08T14:26:01.850448","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# copied from sample code\n","# Input data files are available in the read-only \"../input/\" directory.\n","\n","# on kaggle\n","src = '../input/image-matching-challenge-2022/train'\n","\n","# on pc\n","# src = './image-matching-challenge-2022/train'\n","\n","val_scenes = []\n","for f in os.scandir(src):\n","    if f.is_dir():\n","        cur_scene = os.path.split(f)[-1]\n","        print(f'Found scene \"{cur_scene}\"\" at {f.path}')\n","        val_scenes += [cur_scene]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T13:43:21.128167Z","iopub.status.busy":"2022-04-09T13:43:21.127891Z","iopub.status.idle":"2022-04-09T13:43:28.798108Z","shell.execute_reply":"2022-04-09T13:43:28.797216Z","shell.execute_reply.started":"2022-04-09T13:43:21.128138Z"},"papermill":{"duration":7.599256,"end_time":"2022-04-08T14:26:09.482995","exception":false,"start_time":"2022-04-08T14:26:01.883739","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Each scene in the validation set contains a list of images, poses, and pairs. Let's pick one and look at some images.\n","\n","scene = 'piazza_san_marco'\n","\n","images_dict = {}\n","for filename in glob(f'{src}/{scene}/images/*.jpg'):\n","    cur_id = os.path.basename(os.path.splitext(filename)[0])\n","\n","    # OpenCV expects BGR, but the images are encoded in standard RGB, so you need to do color conversion if you use OpenCV for I/O.\n","    images_dict[cur_id] = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n","    \n","print(f'Loaded {len(images_dict)} images.')\n","\n","num_rows = 6\n","num_cols = 4\n","f, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20), constrained_layout=True)\n","for i, key in enumerate(images_dict):\n","    if i >= num_rows * num_cols:\n","        break\n","    cur_ax = axes[i % num_rows, i // num_rows]\n","    cur_ax.imshow(images_dict[key])\n","    cur_ax.set_title(key)\n","    cur_ax.axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-04-09T14:32:54.022813Z","iopub.status.idle":"2022-04-09T14:32:54.023145Z","shell.execute_reply":"2022-04-09T14:32:54.022993Z","shell.execute_reply.started":"2022-04-09T14:32:54.022977Z"},"trusted":true},"outputs":[],"source":["# dataset\n","class IMC_dataset(Dataset):\n","    \"\"\"Image Matching Challenge 2022 dataset\"\"\"\n","\n","    def __init__(self, csv_file, root_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.landmarks_frame = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.landmarks_frame)\n","\n","    def __getitem__(self, idx):\n","                \"\"\"\n","        Returns:\n","            image_one (tensor): First image\n","            image_two (tensor): Secound image\n","            covisibility (float)\n","            K (matrix): Camera intransic matrix\n","            R (matrix): Rotation matrix\n","            T (vector): Translation vector\n","        \"\"\"\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,\n","                                self.landmarks_frame.iloc[idx, 0])\n","        image = io.imread(img_name)\n","        landmarks = self.landmarks_frame.iloc[idx, 1:]\n","        landmarks = np.array([landmarks])\n","        landmarks = landmarks.astype('float').reshape(-1, 2)\n","        sample = {'image': image, 'landmarks': landmarks}\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T14:32:59.475677Z","iopub.status.busy":"2022-04-09T14:32:59.475138Z","iopub.status.idle":"2022-04-09T14:32:59.51054Z","shell.execute_reply":"2022-04-09T14:32:59.509448Z","shell.execute_reply.started":"2022-04-09T14:32:59.475646Z"},"papermill":{"duration":0.098892,"end_time":"2022-04-08T14:26:09.68268","exception":false,"start_time":"2022-04-08T14:26:09.583788","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# pytorch data loading\n","\n","def get_scene_trainloader(scene):\n","\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","    trainset = \"./image-matching-challenge-2022/train/\" + scene\n","\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n","                                            shuffle=True, num_workers=2)\n","\n","    classes = ('plane', 'car', 'bird', 'cat',\n","            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","    def imshow(img):\n","        img = img / 2 + 0.5     # unnormalize\n","        npimg = img.numpy()\n","        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","\n","    # get some random training images\n","    dataiter = iter(trainloader)\n","\n","    print(dataiter.next())\n","    images, labels = dataiter.next()\n","\n","    # show images\n","    imshow(torchvision.utils.make_grid(images))\n","    # print labels\n","    print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n","\n","    return trainloader\n","\n","get_scene_trainloader(\"piazza_san_marco\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# image manipulation (ref. SiT)\n","# paper:\n","# github: github.com/Sara-Ahmed/SiT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T14:26:09.885364Z","iopub.status.busy":"2022-04-08T14:26:09.884364Z","iopub.status.idle":"2022-04-08T14:26:09.8879Z","shell.execute_reply":"2022-04-08T14:26:09.888436Z"},"papermill":{"duration":0.10691,"end_time":"2022-04-08T14:26:09.888627","exception":false,"start_time":"2022-04-08T14:26:09.781717","status":"completed"},"tags":[]},"outputs":[],"source":["# Summary: A combination of ResNeXt and Squeeze and Excitation Network (SENet) to \n","# ResNeXt ref: https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n","# SENet ref: https://github.com/moskomule/senet.pytorch \n","\n","def conv1x1(in_channels, out_channels, stride = 1):\n","    \"\"\" 1x1 convolution\"\"\"\n","    return nn.Conv2d(\n","        in_channels, out_channels,\n","        kernel_size = 1, \n","        stride = stride,\n","        bias = False\n","    )\n","\n","def conv3x3(in_channels, out_channels, stride = 1, groups = 1, dilation = 1):\n","    \"\"\" 3x3 convolution \"\"\"\n","    return nn.Conv2d(\n","        in_channels, out_channels,\n","        kernel_size = 3,\n","        stride = stride,\n","        padding = dilation,\n","        groups = groups,\n","        bias = False,\n","        dilation = dilation \n","    )\n","\n","class BottleNeck(nn.Module):\n","    \"\"\" BottleNeck Layer in ResNet \"\"\"\n","\n","    expansion : int = 4 \n","\n","    def __init__(\n","        self, in_channels, out_channels, \n","        reduction=2, stride=1, downsample=None, num_groups=64\n","    ):\n","        # if we want to use the expansion factor, we can just modify \"out_channels\"\n","        # out_channels = in_channels * self.expansion\n","\n","        super().__init__()\n","\n","        width = int(in_channels/reduction)\n","\n","        # inplace is used for ReLU to reduce memory usage\n","        self.resnext_block = nn.Sequential(\n","            conv1x1(in_channels, width),\n","            nn.BatchNorm2d(width),\n","            nn.ReLU(inplace=True),\n","\n","            conv3x3(width, width, stride=stride, groups=num_groups),\n","            nn.BatchNorm2d(width),\n","            nn.ReLU(inplace=True),\n","\n","            conv1x1(width, out_channels),\n","            norm_layer(out_channels)\n","        )\n","\n","        if downsample is not None:\n","            self.downsample = downsample\n","\n","        self.activation = nn.ReLU(inplace=True)\n","    \n","    def forward(self, x):\n","        residual = x \n","        out = self.resnext_block(x)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","            \n","        out += residual\n","        out = self.activation(out)\n","        return out\n","\n","class SELayer(nn.Module):\n","    \"\"\" building block described in the SENet Paper and github\"\"\"\n","    def __init__(self, channel, reduction=16):\n","        super().__init__())\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Sequential(\n","            nn.Linear(channel, in_channel//reduction, bias=False),\n","            nn.ReLU(inplace=True)\n","            nn.Linear(in_channel//reduction, channel, bias=False),\n","            nn.Sigmoid()\n","        )\n","    \n","    def forward(self, x):\n","        batch, channel, _, _ = x.size()\n","        y = self.avg_pool(x).view(batch, channel)\n","        y = self.fc(y).view(batch, channel, 1, 1)\n","        return x * y.expand_as(x)\n","\n","class ResNeXtLayer(nn.Module):\n","    \"\"\" basic layer described in the ResNeXt Paper \"\"\"\n","    def __init__(self, in_channels, out_channels, num_blocks, num_groups=64, dilation=1, stride=1):\n","        super().__init__()\n","        downsample = nn.Sequential(\n","            conv1x1(in_channels, out_channels, stride),\n","            nn.BatchNorm2d(out_channels)\n","        )\n","\n","        self.layers = []\n","        self.layers.append(BottleNeck(in_channels, out_channels, downsample=downsample, stride=stride))\n","        \n","        for _ in range(1, num_blocks):\n","            self.layers.append(\n","                BottleNeck(out_channels, out_channels, dilation=dilation, num_groups=num_groups)\n","            )\n","    def forward(self, x):\n","        x = self.layers(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# CAPE: Continuous Augmented Positional Embeddings\n","# paper @ https://arxiv.org/2106.03143\n","\n","class CAPE(nn.Module): # I have offically given up on encoding in general\n","    def __init__(\n","        self, model_dim \n","        max_global_shift = 0.0, max_local_shift = 0.0, max_global_scaling = 1.0, \n","    ):\n","        self.max_global_shift = max_global_shift\n","        self.max_local_shift = max_local_shift\n","        self.max_global_scaling = max_global_scaling\n","\n","        self.register_buffer('content_scale', nn.Tensor([math.sqrt(model_dim)]))\n","\n","    def forward(self, patches):\n","        return (patches * self.content_scale) + self.compute_pos_emb(patches)\n","    \n","    def compute_pos_emb(self, patches):\n","        batch, height, width, channel = patches.shape()\n","\n","        x = torch.zeros\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TransCNN + Universal Transformer \n","# Idea : each layer repeated via ACT, then down sampled\n","# Idea : direct downsampling for residual\n","\n","class CNNAttention(nn.Module):\n","    def __init__(self, total_dim, head_dim, grid_size=1, downsample_rate=1, drop=0):\n","        super().__init__()\n","        self.num_heads = total_dim // head_dim # area of previous step / area of head\n","        self.head_dim = head_dim \n","        self.side_len = self.head_dim ** -0.5\n","        self.grid_size = grid_size\n","\n","        self.norm = nn.BatchNorm2d()\n","        self.qkv = conv1x1(total_dim, total_dim * 3)\n","        self.proj = nn.Conv2d(total_dim, total_dim)\n","        self.drop = nn.Dropout2d(drop, inplace=True)\n","\n","        if self.grid_size > 1:\n","            self.q = conv1x1(total_dim, total_dim)\n","            self.kv = conv1x1(total_dim, total_dim * 2)\n","\n","            self.grid_norm = nn.BatchNorm2d(total_dim)\n","            self.avg_pool = nn.AvgPool2d(total_dim)\n","            self.downsample_norm = nn.BatchNorm2d(total_dim)\n","\n","    def forward(self, x):\n","        batch, channels, height, width = x.shape()\n","        qkv = self.qkv(self.norm(x))\n","\n","        if self.grid_size > 1:\n","            # compute grid based/local attention\n","            grid_h, grid_w = height // self.grid_size, width // self.grid_size # H/G, W/G\n","            qkv = qkv.reshape(\n","                batch, 3, # q, k, v\n","                self.num_heads, self.head_dim, \n","                grid_h, self.grid_size,\n","                grid_w, self.grid_size\n","            ) # ref. the dimensions of this space is R^ Batch * QKV * Head * Size * H/G * G * W/G * G\n","            qkv = qkv.permute(1, 0, 2, 4, 6, 5, 7, 3) # R^ QKV * Batch * Head * H/G * W/G * G * G * Size \n","            qkv = qkv.reshape(3, -1, self.grid_size ** 2, self.head_dim) # R^ QKV * (Batch * Head * H/ * G * G * Size \n","            q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","            attn = (q @ k.transpose(-2, -1)) * self.side_len # transpose k -> R^ Batch * G * G * (W * H)\n","            attn = attn.softmax(dim=-1)\n","            grid_x = (attn @ v).reshape(\n","                batch, self.num_heads, \n","                grid_h, grid_w, \n","                self.grid_size, self.grid_size, \n","                self.head_dim \n","            ) # R^ Batch * Head * H/G * W/G * G * G * Size, same as after permute\n","            grid_x = self.grid_norm(x + grid_x) #residue and normalisation\n","\n","            # transform qkv for computing global attention\n","            q = self.q(grid_x).reshape(batch, self.num_heads, self.head_dim, -1) # R^ Batch * Head * Size * (H * W)\n","            q = q.transpose(-2, -1) # R^ Batch * Head * (H * W) * Size \n","            kv = self.kv(self.downsample_norm(self.avg_pool(grid_x)))\n","            kv = kv.reshape(batch, 2, self.num_heads, self.head_dim, -1) # R^ Batch * KV * Head * Size * (H * W)\n","            kv = kv.permute(1, 0, 2, 4, 3) # R^ KV * Batch * Head *  (H * W) * Size \n","            k, v = kv[0], kv[1] # R^ Batch * Head * (H * W) * Size\n","        else: \n","            # transform qkv for computing global attention\n","            qkv = qkv.reshape(B, 3, self.num_heads, self.head_dim, -1) # R^ Batch * QKV * Head * Size * (H * W)\n","            qkv = qkv.permute(1, 0, 2, 4, 3) # R^ QKV * Batch * Head * (H * W) * Size\n","            q, k, v = qkv[0], qkv[1], qkv[2]\n","        \n","        # compute global attention\n","        attn = (q @ k.transpose(-2, -1)) * self.side_len \n","        attn = attn.softmax(dim=1)\n","        global_x = (attn @ v).transpose(-2, -1).reshape(batch, channel, height, width)\n","\n","        # residue\n","        if self.grid_size > 1\n","            global_x += grid_x\n","        x = self.drop(self.proj(global_x))\n","\n","        return x \n","\n","class ACT(nn.Module):\n","\n","    threshold = 1 - 0.1\n","\n","    def __init__(self, size, activation=nn.Sigmoid):\n","        super().__init__()\n","        self.activation = activation()\n","        self.fc = nn.Linear(size, 1) # What if we replace linear with conv2d?\n","        # !!!be sure to initialise self.p!!!\n","    \n","    def forward(self, state, inputs, fn, max_steps):\n","        # input flattened from 4d to 3d for ACT\n","        batch, size, _ = inputs.shape()\n","\n","        halting_probablity = torch.zeros(batch, size)\n","        remainders = torch.zeros(batch, size)\n","        n_updates = torch.zeros(batch, size)\n","        previous_state = torch.zeros_like(inputs)\n","\n","        def should_continue(h, n, m):\n","            return (h < self.threshold and n < m).byte().any()\n","        while should_continue(halting_probability, n_updates, max_steps):\n","            # we are avoiding timing signals because we have our own RPE \n","\n","            state = self.activation(self.fc(state)).squeeze(-1) \n","\n","            # calculate masks for which ones to halt\n","            still_running = (halting_probability < 1.0).float()\n","            new_halted = (halting_probability + state * still_running > self.threshold).float() * still_running\n","            still_running = (halting_probability + state * still_running <= self.threshold).float() * still_running\n","\n","            # halt parameters and increment remainders\n","            halting_probability += state * still_running\n","            remainders += new_halted * (1 - halting_probability)\n","            halting_probability += new_halted * remainders \n","            n_updates += still_running + new_halted\n","            # compute weights to apply to the state and output\n","            update_weights = state * still_running + new_halted * remainders \n","            \n","            state = fn(state)\n","            previous_state = state * update_weights.unsqueeze(-1) + previous_state * (1 - update_weights.unsqueeze(-1))\n","\n","            step += 1\n","        return previous_state, (remainders, n_updates)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for decoder architecture we are going to reuse existing models \n","# MobileStyleGAN \n","# Q: Can we just ctrl c+p this?\n","# A: Yes! Yes we can!\n","\n","\n","from pytorch_wavelets.dwt.lowlevel import *\n","\n","def _SFB2D(low, highs, g0_row, g1_row, g0_col, g1_col, mode):\n","    mode = int_to_mode(mode)\n","\n","    lh, hl, hh = torch.unbind(highs, dim=2)\n","    lo = sfb1d(low, lh, g0_col, g1_col, mode=mode, dim=2)\n","    hi = sfb1d(hl, hh, g0_col, g1_col, mode=mode, dim=2)\n","    y = sfb1d(lo, hi, g0_row, g1_row, mode=mode, dim=3)\n","\n","    return y\n","\n","class DWTInverse(nn.Module):\n","    \"\"\" Performs a 2d DWT Inverse reconstruction of an image\n","    Args:\n","        wave (str or pywt.Wavelet): Which wavelet to use\n","        C: deprecated, will be removed in future\n","    \"\"\"\n","    def __init__(self, wave='db1', mode='zero', trace_model=False):\n","        super().__init__()\n","        if isinstance(wave, str):\n","            wave = pywt.Wavelet(wave)\n","        if isinstance(wave, pywt.Wavelet):\n","            g0_col, g1_col = wave.rec_lo, wave.rec_hi\n","            g0_row, g1_row = g0_col, g1_col\n","        else:\n","            if len(wave) == 2:\n","                g0_col, g1_col = wave[0], wave[1]\n","                g0_row, g1_row = g0_col, g1_col\n","            elif len(wave) == 4:\n","                g0_col, g1_col = wave[0], wave[1]\n","                g0_row, g1_row = wave[2], wave[3]\n","        # Prepare the filters\n","        filts = prep_filt_sfb2d(g0_col, g1_col, g0_row, g1_row)\n","        self.register_buffer('g0_col', filts[0])\n","        self.register_buffer('g1_col', filts[1])\n","        self.register_buffer('g0_row', filts[2])\n","        self.register_buffer('g1_row', filts[3])\n","        self.mode = mode\n","        self.trace_model = trace_model\n","\n","    def forward(self, coeffs):\n","        yl, yh = coeffs\n","        ll = yl\n","        mode = mode_to_int(self.mode)\n","\n","        for h in yh[::-1]:\n","            if h is None:\n","                h = torch.zeros(ll.shape[0], ll.shape[1], 3, ll.shape[-2],\n","                                ll.shape[-1], device=ll.device)\n","\n","            if ll.shape[-2] > h.shape[-2]:\n","                ll = ll[...,:-1,:]\n","            if ll.shape[-1] > h.shape[-1]:\n","                ll = ll[...,:-1]\n","            if not self.trace_model:\n","                ll = SFB2D.apply(ll, h, self.g0_col, self.g1_col, self.g0_row, self.g1_row, mode)\n","            else:\n","                ll = _SFB2D(ll, h, self.g0_col, self.g1_col, self.g0_row, self.g1_row, mode)\n","        return ll\n","\n","class IDWTUpsaplme(nn.Module):\n","    def __init__(\n","            self,\n","            channels_in,\n","            style_dim,\n","    ):\n","        super().__init__()\n","        self.channels = channels_in // 4\n","        assert self.channels * 4 == channels_in\n","        # upsample\n","        self.idwt = DWTInverse(mode='zero', wave='db1')\n","        # modulation\n","        self.modulation = nn.Linear(style_dim, channels_in, bias=True)\n","        self.modulation.bias.data.fill_(1.0)\n","\n","    def forward(self, x, style):\n","        b, _, h, w = x.size()\n","        x = self.modulation(style).view(b, -1, 1, 1) * x\n","        low = x[:, :self.channels]\n","        high = x[:, self.channels:]\n","        high = high.view(b, self.channels, 3, h, w)\n","        x = self.idwt((low, [high]))\n","        return x\n","\n","class ModulatedConv2d(nn.Module):\n","    def __init__(\n","            self,\n","            channels_in,\n","            channels_out,\n","            style_dim,\n","            kernel_size,\n","            demodulate=True\n","    ):\n","        super().__init__()\n","        # create conv\n","        self.weight = nn.Parameter(\n","            torch.randn(channels_out, channels_in, kernel_size, kernel_size)\n","        )\n","        # create modulation network\n","        self.modulation = nn.Linear(style_dim, channels_in, bias=True)\n","        self.modulation.bias.data.fill_(1.0)\n","        # create demodulation parameters\n","        self.demodulate = demodulate\n","        if self.demodulate:\n","            self.register_buffer(\"style_inv\", torch.randn(1, 1, channels_in, 1, 1))\n","        # some service staff\n","        self.scale = 1.0 / math.sqrt(channels_in * kernel_size ** 2)\n","        self.padding = kernel_size // 2\n","\n","    def forward(self, x, style):\n","        modulation = self.get_modulation(style)\n","        x = modulation * x\n","        x = F.conv2d(x, self.weight, padding=self.padding)\n","        if self.demodulate:\n","            demodulation = self.get_demodulation(style)\n","            x = demodulation * x\n","        return x\n","\n","    def get_modulation(self, style):\n","        style = self.modulation(style).view(style.size(0), -1, 1, 1)\n","        modulation = self.scale * style\n","        return modulation\n","\n","    def get_demodulation(self, style):\n","        w = self.weight.unsqueeze(0)\n","        norm = torch.rsqrt((self.scale * self.style_inv * w).pow(2).sum([2, 3, 4]) + 1e-8)\n","        demodulation = norm\n","        return demodulation.view(*demodulation.size(), 1, 1)\n","\n","\n","class ModulatedDWConv2d(nn.Module):\n","    def __init__(\n","            self,\n","            channels_in,\n","            channels_out,\n","            style_dim,\n","            kernel_size,\n","            demodulate=True\n","    ):\n","        super().__init__()\n","        # create conv\n","        self.weight_dw = nn.Parameter(\n","            torch.randn(channels_in, 1, kernel_size, kernel_size)\n","        )\n","        self.weight_permute = nn.Parameter(\n","            torch.randn(channels_out, channels_in, 1, 1)\n","        )\n","        # create modulation network\n","        self.modulation = nn.Linear(style_dim, channels_in, bias=True)\n","        self.modulation.bias.data.fill_(1.0)\n","        # create demodulation parameters\n","        self.demodulate = demodulate\n","        if self.demodulate:\n","            self.register_buffer(\"style_inv\", torch.randn(1, 1, channels_in, 1, 1))\n","        # some service staff\n","        self.scale = 1.0 / math.sqrt(channels_in * kernel_size ** 2)\n","        self.padding = kernel_size // 2\n","\n","    def forward(self, x, style):\n","        modulation = self.get_modulation(style)\n","        x = modulation * x\n","        x = F.conv2d(x, self.weight_dw, padding=self.padding, groups=x.size(1))\n","        x = F.conv2d(x, self.weight_permute)\n","        if self.demodulate:\n","            demodulation = self.get_demodulation(style)\n","            x = demodulation * x\n","        return x\n","\n","    def get_modulation(self, style):\n","        style = self.modulation(style).view(style.size(0), -1, 1, 1)\n","        modulation = self.scale * style\n","        return modulation\n","\n","    def get_demodulation(self, style):\n","        w = (self.weight_dw.transpose(0, 1) * self.weight_permute).unsqueeze(0)\n","        norm = torch.rsqrt((self.scale * self.style_inv * w).pow(2).sum([2, 3, 4]) + 1e-8)\n","        demodulation = norm\n","        return demodulation.view(*demodulation.size(), 1, 1)\n","\n","class StyledConv2d(nn.Module):\n","    def __init__(\n","        self,\n","        channels_in,\n","        channels_out,\n","        style_dim,\n","        kernel_size,\n","        demodulate=True,\n","        conv_module\n","    ):\n","        super().__init__()\n","\n","        self.conv = conv_module(\n","            channels_in,\n","            channels_out,\n","            style_dim,\n","            kernel_size,\n","            demodulate=demodulate\n","        )\n","\n","        self.noise = NoiseInjection()\n","        self.bias = nn.Parameter(torch.zeros(1, channels_out, 1, 1))\n","        self.act = nn.LeakyReLU(0.2)\n","\n","    def forward(self, input, style, noise=None):\n","        out = self.conv(input, style)\n","        out = self.noise(out, noise=noise)\n","        out = self.act(out + self.bias)\n","        return out\n","\n","class MultichannelImage(nn.Module):\n","    def __init__(\n","            self,\n","            channels_in,\n","            channels_out,\n","            style_dim,\n","            kernel_size=1\n","    ):\n","        super().__init__()\n","        self.conv = ModulatedConv2d(channels_in, channels_out, style_dim, kernel_size, demodulate=False)\n","        self.bias = nn.Parameter(torch.zeros(1, channels_out, 1, 1))\n","\n","    def forward(self, hidden, style):\n","        out = self.conv(hidden, style)\n","        out = out + self.bias\n","        return out\n","\n","class MobileSynthesisBlock(nn.Module):\n","    def __init__(\n","            self,\n","            channels_in,\n","            channels_out,\n","            style_dim,\n","            kernel_size=3,\n","            conv_module\n","    ):\n","        super().__init__()\n","        self.up = IDWTUpsample(channels_in, style_dim)\n","        self.conv1 = StyledConv2d(\n","            channels_in // 4,\n","            channels_out,\n","            style_dim,\n","            kernel_size,\n","            conv_module=conv_module\n","        )\n","        self.conv2 = StyledConv2d(\n","            channels_out,\n","            channels_out,\n","            style_dim,\n","            kernel_size,\n","            conv_module=conv_module\n","        )\n","        self.to_img = MultichannelImage(\n","            channels_in=channels_out,\n","            channels_out=12,\n","            style_dim=style_dim,\n","            kernel_size=1\n","        )\n","\n","    def forward(self, hidden, style, noise=[None, None]):\n","        hidden = self.up(hidden, style if style.ndim == 2 else style[:, 0, :])\n","        hidden = self.conv1(hidden, style if style.ndim == 2 else style[:, 0, :], noise=noise[0])\n","        hidden = self.conv2(hidden, style if style.ndim == 2 else style[:, 1, :], noise=noise[1])\n","        img = self.to_img(hidden, style if style.ndim == 2 else style[:, 2, :])\n","        return hidden, img\n","\n","    def wsize(self):\n","        return 3\n","\n","class MobileSynthesisNetwork(nn.Module):\n","    def __init__(\n","            self,\n","            style_dim,\n","            channels = [512, 512, 512, 512, 512, 256, 128, 64]\n","    ):\n","        super().__init__()\n","        self.style_dim = style_dim\n","\n","        self.input = ConstantInput(channels[0])\n","        self.conv1 = StyledConv2d(\n","            channels[0],\n","            channels[0],\n","            style_dim,\n","            kernel_size=3\n","        )\n","        self.to_img1 = MultichannelImage(\n","            channels_in=channels[0],\n","            channels_out=12,\n","            style_dim=style_dim,\n","            kernel_size=1\n","        )\n","\n","        self.layers = nn.ModuleList()\n","        channels_in = channels[0]\n","        for i, channels_out in enumerate(channels[1:]):\n","            self.layers.append(\n","                MobileSynthesisBlock(\n","                    channels_in,\n","                    channels_out,\n","                    style_dim,\n","                    3,\n","                    conv_module=ModulatedDWConv2d\n","                )\n","            )\n","            channels_in = channels_out\n","\n","        self.idwt = DWTInverse(mode=\"zero\", wave=\"db1\")\n","        self.register_buffer(\"device_info\", torch.zeros(1))\n","        self.trace_model = False\n","\n","    def forward(self, style, noise=None):\n","        out = {\"noise\": [], \"freq\": [], \"img\": None}\n","        noise = NoiseManager(noise, self.device_info.device, self.trace_model)\n","\n","        hidden = self.input(style)\n","        out[\"noise\"].append(noise(hidden.size(-1)))\n","        hidden = self.conv1(hidden, style if style.ndim == 2 else style[:, 0, :], noise=out[\"noise\"][-1])\n","        img = self.to_img1(hidden, style if style.ndim == 2 else style[:, 1, :])\n","        out[\"freq\"].append(img)\n","\n","        for i, m in enumerate(self.layers):\n","            out[\"noise\"].append(noise(2 ** (i + 3), 2))\n","            _style = style if style.ndim == 2 else style[:, m.wsize()*i + 1: m.wsize()*i + m.wsize() + 1, :]\n","            hidden, freq = m(hidden, _style, noise=out[\"noise\"][-1])\n","            out[\"freq\"].append(freq)\n","\n","        out[\"img\"] = self.dwt_to_img(out[\"freq\"][-1])\n","        return out\n","\n","    def dwt_to_img(self, img):\n","        b, c, h, w = img.size()\n","        low = img[:, :3, :, :]\n","        high = img[:, 3:, :, :].view(b, 3, 3, h, w)\n","        return self.idwt((low, [high]))\n","\n","    def wsize(self):\n","        return len(self.layers) * self.layers[0].wsize() + 2"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
