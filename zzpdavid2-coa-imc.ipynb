{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-09T13:42:23.040019Z","iopub.status.busy":"2022-04-09T13:42:23.039666Z","iopub.status.idle":"2022-04-09T13:42:23.048228Z","shell.execute_reply":"2022-04-09T13:42:23.047033Z","shell.execute_reply.started":"2022-04-09T13:42:23.039983Z"},"papermill":{"duration":0.405206,"end_time":"2022-04-08T14:26:01.787265","exception":false,"start_time":"2022-04-08T14:26:01.382059","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# imports\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","import csv\n","\n","from glob import glob\n","import matplotlib.pyplot as plt\n","from collections import namedtuple\n","from copy import deepcopy\n","from tqdm import tqdm\n","import random\n","\n","import cv2\n","\n","# pytorch + torchvision\n","import torch \n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","\n","# Check that you're using a recent OpenCV version.\n","assert cv2.__version__ > '4.5', 'Please use OpenCV 4.5 or later.'"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007766,"end_time":"2022-04-08T14:26:01.803635","exception":false,"start_time":"2022-04-08T14:26:01.795869","status":"completed"},"tags":[]},"source":["Write down the plan in words and references. We will then fill in code."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T14:26:01.824321Z","iopub.status.busy":"2022-04-08T14:26:01.823683Z","iopub.status.idle":"2022-04-08T14:26:01.82526Z","shell.execute_reply":"2022-04-08T14:26:01.825701Z"},"papermill":{"duration":0.014252,"end_time":"2022-04-08T14:26:01.82586","exception":false,"start_time":"2022-04-08T14:26:01.811608","status":"completed"},"tags":[]},"outputs":[],"source":["# Encoding: ResNeXt (conv 1-3) => Position Enc. (ref. 2107.14222) => Deep-ViT (w/ EfficientNet)\n","# Decoding(for unsupervised training) : HiT(low resolution stage, same # as Deep-ViT) => FCC-GAN\n","# Comparison : Transformer-XL \n","# Classifier/MLP : MLP head (output 8) => Reconstruction module => F\n","\n","# Training Steps:\n","# 1. Train Encoder (unsupervised): manipulate input image (ref. SiT, + rotation) and match to output\n","# 2. Train Comparison (unsupervised) : use different head, mix&match the 2 images (ref. BERT, ALBERT)\n","# 3. Train MLP (supervised) : compare output to F\n","\n","# Inference Steps: Encode each image => concat. 2 images => Comparison => MLP\n","\n","#offical sample code: https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.007731,"end_time":"2022-04-08T14:26:01.842112","exception":false,"start_time":"2022-04-08T14:26:01.834381","status":"completed"},"tags":[]},"source":["# Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T13:43:17.302881Z","iopub.status.busy":"2022-04-09T13:43:17.302548Z","iopub.status.idle":"2022-04-09T13:43:17.319887Z","shell.execute_reply":"2022-04-09T13:43:17.319024Z","shell.execute_reply.started":"2022-04-09T13:43:17.302849Z"},"papermill":{"duration":0.02399,"end_time":"2022-04-08T14:26:01.874438","exception":false,"start_time":"2022-04-08T14:26:01.850448","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# copied from sample code\n","# Input data files are available in the read-only \"../input/\" directory.\n","\n","# on kaggle\n","src = '../input/image-matching-challenge-2022/train'\n","\n","# on pc\n","# src = './image-matching-challenge-2022/train'\n","\n","val_scenes = []\n","for f in os.scandir(src):\n","    if f.is_dir():\n","        cur_scene = os.path.split(f)[-1]\n","        print(f'Found scene \"{cur_scene}\"\" at {f.path}')\n","        val_scenes += [cur_scene]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T13:43:21.128167Z","iopub.status.busy":"2022-04-09T13:43:21.127891Z","iopub.status.idle":"2022-04-09T13:43:28.798108Z","shell.execute_reply":"2022-04-09T13:43:28.797216Z","shell.execute_reply.started":"2022-04-09T13:43:21.128138Z"},"papermill":{"duration":7.599256,"end_time":"2022-04-08T14:26:09.482995","exception":false,"start_time":"2022-04-08T14:26:01.883739","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Each scene in the validation set contains a list of images, poses, and pairs. Let's pick one and look at some images.\n","\n","scene = 'piazza_san_marco'\n","\n","images_dict = {}\n","for filename in glob(f'{src}/{scene}/images/*.jpg'):\n","    cur_id = os.path.basename(os.path.splitext(filename)[0])\n","\n","    # OpenCV expects BGR, but the images are encoded in standard RGB, so you need to do color conversion if you use OpenCV for I/O.\n","    images_dict[cur_id] = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n","    \n","print(f'Loaded {len(images_dict)} images.')\n","\n","num_rows = 6\n","num_cols = 4\n","f, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20), constrained_layout=True)\n","for i, key in enumerate(images_dict):\n","    if i >= num_rows * num_cols:\n","        break\n","    cur_ax = axes[i % num_rows, i // num_rows]\n","    cur_ax.imshow(images_dict[key])\n","    cur_ax.set_title(key)\n","    cur_ax.axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-04-09T14:32:54.022813Z","iopub.status.idle":"2022-04-09T14:32:54.023145Z","shell.execute_reply":"2022-04-09T14:32:54.022993Z","shell.execute_reply.started":"2022-04-09T14:32:54.022977Z"},"trusted":true},"outputs":[],"source":["# dataset\n","class IMC_dataset(Dataset):\n","    \"\"\"Image Matching Challenge 2022 dataset\"\"\"\n","\n","    def __init__(self, csv_file, root_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.landmarks_frame = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.landmarks_frame)\n","\n","    def __getitem__(self, idx):\n","                \"\"\"\n","        Returns:\n","            image_one (tensor): First image\n","            image_two (tensor): Secound image\n","            covisibility (float)\n","            K (matrix): Camera intransic matrix\n","            R (matrix): Rotation matrix\n","            T (vector): Translation vector\n","        \"\"\"\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,\n","                                self.landmarks_frame.iloc[idx, 0])\n","        image = io.imread(img_name)\n","        landmarks = self.landmarks_frame.iloc[idx, 1:]\n","        landmarks = np.array([landmarks])\n","        landmarks = landmarks.astype('float').reshape(-1, 2)\n","        sample = {'image': image, 'landmarks': landmarks}\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T14:32:59.475677Z","iopub.status.busy":"2022-04-09T14:32:59.475138Z","iopub.status.idle":"2022-04-09T14:32:59.51054Z","shell.execute_reply":"2022-04-09T14:32:59.509448Z","shell.execute_reply.started":"2022-04-09T14:32:59.475646Z"},"papermill":{"duration":0.098892,"end_time":"2022-04-08T14:26:09.68268","exception":false,"start_time":"2022-04-08T14:26:09.583788","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# pytorch data loading\n","\n","def get_scene_trainloader(scene):\n","\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","    trainset = \"./image-matching-challenge-2022/train/\" + scene\n","\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n","                                            shuffle=True, num_workers=2)\n","\n","    classes = ('plane', 'car', 'bird', 'cat',\n","            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","    def imshow(img):\n","        img = img / 2 + 0.5     # unnormalize\n","        npimg = img.numpy()\n","        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","\n","    # get some random training images\n","    dataiter = iter(trainloader)\n","\n","    print(dataiter.next())\n","    images, labels = dataiter.next()\n","\n","    # show images\n","    imshow(torchvision.utils.make_grid(images))\n","    # print labels\n","    print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n","\n","    return trainloader\n","\n","get_scene_trainloader(\"piazza_san_marco\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# image manipulation (ref. SiT)\n","# paper:\n","# github: github.com/Sara-Ahmed/SiT"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T14:26:09.885364Z","iopub.status.busy":"2022-04-08T14:26:09.884364Z","iopub.status.idle":"2022-04-08T14:26:09.8879Z","shell.execute_reply":"2022-04-08T14:26:09.888436Z"},"papermill":{"duration":0.10691,"end_time":"2022-04-08T14:26:09.888627","exception":false,"start_time":"2022-04-08T14:26:09.781717","status":"completed"},"tags":[]},"outputs":[],"source":["# Summary: A combination of ResNeXt and Squeeze and Excitation Network (SENet) to \n","# ResNeXt ref: https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n","# SENet ref: https://github.com/moskomule/senet.pytorch \n","\n","def conv1x1(in_channels, out_channels, stride = 1):\n","    \"\"\" 1x1 convolution\"\"\"\n","    return nn.Conv2d(\n","        in_channels, out_channels,\n","        kernel_size = 1, \n","        stride = stride,\n","        bias = False\n","    )\n","\n","def conv3x3(in_channels, out_channels, stride = 1, groups = 1, dilation = 1):\n","    \"\"\" 3x3 convolution \"\"\"\n","    return nn.Conv2d(\n","        in_channels, out_channels,\n","        kernel_size = 3,\n","        stride = stride,\n","        padding = dilation,\n","        groups = groups,\n","        bias = False,\n","        dilation = dilation \n","    )\n","\n","class BottleNeck(nn.Module):\n","    \"\"\" BottleNeck Layer in ResNet \"\"\"\n","\n","    expansion : int = 4 \n","\n","    def __init__(\n","        self, in_channels, out_channels, \n","        reduction=2, stride=1, downsample=None, num_groups=64\n","    ):\n","        # if we want to use the expansion factor, we can just modify \"out_channels\"\n","        # out_channels = in_channels * self.expansion\n","\n","        super().__init__()\n","\n","        width = int(in_channels/reduction)\n","\n","        # inplace is used for ReLU to reduce memory usage\n","        self.resnext_block = nn.Sequential(\n","            conv1x1(in_channels, width),\n","            nn.BatchNorm2d(width),\n","            nn.ReLU(inplace=True),\n","\n","            conv3x3(width, width, stride=stride, groups=num_groups),\n","            nn.BatchNorm2d(width),\n","            nn.ReLU(inplace=True),\n","\n","            conv1x1(width, out_channels),\n","            norm_layer(out_channels)\n","        )\n","\n","        if downsample is not None:\n","            self.downsample = downsample\n","\n","        self.activation = nn.ReLU(inplace=True)\n","    \n","    def forward(self, x):\n","        residual = x \n","        out = self.resnext_block(x)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","            \n","        out += residual\n","        out = self.activation(out)\n","        return out\n","\n","class SELayer(nn.Module):\n","    \"\"\" building block described in the SENet Paper and github\"\"\"\n","    def __init__(self, channel, reduction=16):\n","        super().__init__())\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Sequential(\n","            nn.Linear(channel, in_channel//reduction, bias=False),\n","            nn.ReLU(inplace=True)\n","            nn.Linear(in_channel//reduction, channel, bias=False),\n","            nn.Sigmoid()\n","        )\n","    \n","    def forward(self, x):\n","        batch, channel, _, _ = x.size()\n","        y = self.avg_pool(x).view(batch, channel)\n","        y = self.fc(y).view(batch, channel, 1, 1)\n","        return x * y.expand_as(x)\n","\n","class ResNeXtLayer(nn.Module):\n","    \"\"\" basic layer described in the ResNeXt Paper \"\"\"\n","    def __init__(self, in_channels, out_channels, num_blocks, num_groups=64, dilation=1, stride=1):\n","        super().__init__()\n","        downsample = nn.Sequential(\n","            conv1x1(in_channels, out_channels, stride),\n","            nn.BatchNorm2d(out_channels)\n","        )\n","\n","        layers = []\n","        layers.append(BottleNeck(in_channels, out_channels, downsample=downsample, stride=stride))\n","        \n","        for _ in range(1, num_blocks):\n","            layers.append(\n","                BottleNeck(out_channels, out_channels, dilation=dilation, num_groups=num_groups)\n","            )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Relative Position Encoding\n","# paper: arxiv.org/pdf/2107.14222.pdf\n","\n","class RPE(nn.Module):\n","    \"\"\" implementation of the product method in the paper \"\"\"\n","    def __init__():\n","        pass\n","\n","# TransCNN + Universal Transformer \n","# Idea : each layer repeated via ACT, then down sampled\n","# Idea : direct downsampling for residual"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
