{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install torch==1.11.0\n# !pip install torchvision==0.12.0\n# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl\n\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version 1.11 --apt-packages libomp5 libopenblas-dev\n\n# on TPU\n# !pip install torch==1.9.0\n!pip install torchvision==0.10.0\n!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n    \n!pip install git+https://github.com/fbcotter/pytorch_wavelets \n\n#!pip install cloud-tpu-client --upgrade\n\n    \n# import maths\nimport sys\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom numpy.random import randint  \n\nimport os\nimport csv\n\nimport glob\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple\nfrom copy import deepcopy\nfrom tqdm import tqdm\nimport random\n\nimport cv2\n\n# pytorch + torchvision\nimport torch \nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter \nimport torch.nn.functional as F\n\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torchvision.io as io\n\nimport kornia\n\n# Check that you're using a recent OpenCV version.\nassert cv2.__version__ > '4.5', 'Please use OpenCV 4.5 or later.'\n\nprint (sys.version)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.405206,"end_time":"2022-04-08T14:26:01.787265","exception":false,"start_time":"2022-04-08T14:26:01.382059","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-13T03:24:29.808986Z","iopub.execute_input":"2022-05-13T03:24:29.809326Z","iopub.status.idle":"2022-05-13T03:26:07.901991Z","shell.execute_reply.started":"2022-05-13T03:24:29.809248Z","shell.execute_reply":"2022-05-13T03:26:07.901071Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchvision==0.10.0\n  Downloading torchvision-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n\u001b[K     |████████████████████████████████| 22.1 MB 2.3 MB/s eta 0:00:01\n\u001b[?25hCollecting torch==1.9.0\n  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n\u001b[K     |████████████████████████████████| 831.4 MB 1.9 kB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.10.0) (8.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.10.0) (1.19.5)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.9.0->torchvision==0.10.0) (3.7.4.3)\nInstalling collected packages: torch, torchvision\n  Attempting uninstall: torch\n    Found existing installation: torch 1.7.1+cpu\n    Uninstalling torch-1.7.1+cpu:\n      Successfully uninstalled torch-1.7.1+cpu\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.8.2+cpu\n    Uninstalling torchvision-0.8.2+cpu:\n      Successfully uninstalled torchvision-0.8.2+cpu\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchtext 0.8.1 requires torch==1.7.1, but you have torch 1.9.0 which is incompatible.\ntorchaudio 0.7.2 requires torch==1.7.1, but you have torch 1.9.0 which is incompatible.\nfastai 2.2.7 requires torch<1.8,>=1.7.0, but you have torch 1.9.0 which is incompatible.\nfastai 2.2.7 requires torchvision<0.9,>=0.8, but you have torchvision 0.10.0 which is incompatible.\u001b[0m\nSuccessfully installed torch-1.9.0 torchvision-0.10.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting torch-xla==1.9\n  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9 MB)\n\u001b[K     |████████████████████████████████| 149.9 MB 31 kB/s s eta 0:00:01\n\u001b[?25hRequirement already satisfied: cloud-tpu-client==0.10 in /opt/conda/lib/python3.7/site-packages (0.10)\nRequirement already satisfied: google-api-python-client==1.8.0 in /opt/conda/lib/python3.7/site-packages (from cloud-tpu-client==0.10) (1.8.0)\nRequirement already satisfied: oauth2client in /opt/conda/lib/python3.7/site-packages (from cloud-tpu-client==0.10) (4.1.3)\nRequirement already satisfied: six<2dev,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\nRequirement already satisfied: google-auth>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.34.0)\nRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\nRequirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.1.0)\nRequirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.19.1)\nRequirement already satisfied: google-api-core<2dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.31.1)\nRequirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.18.0)\nRequirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.0)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.1)\nRequirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.25.1)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.7.2)\nRequirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.6)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\nInstalling collected packages: torch-xla\nSuccessfully installed torch-xla-1.9\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nCollecting git+https://github.com/fbcotter/pytorch_wavelets\n  Cloning https://github.com/fbcotter/pytorch_wavelets to /tmp/pip-req-build-5nvcssyl\n  Running command git clone -q https://github.com/fbcotter/pytorch_wavelets /tmp/pip-req-build-5nvcssyl\n  Resolved https://github.com/fbcotter/pytorch_wavelets to commit 8d2e3b4289beaea9aa89f7b1dbb290e448331197\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pytorch-wavelets==1.3.0) (1.19.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from pytorch-wavelets==1.3.0) (1.15.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pytorch-wavelets==1.3.0) (1.9.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->pytorch-wavelets==1.3.0) (3.7.4.3)\nBuilding wheels for collected packages: pytorch-wavelets\n  Building wheel for pytorch-wavelets (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorch-wavelets: filename=pytorch_wavelets-1.3.0-py3-none-any.whl size=54869 sha256=575d91db1907db49f2c4e2c5a897227da93535e6439c4f56aabf04a3ce395c2e\n  Stored in directory: /tmp/pip-ephem-wheel-cache-de1xm6xb/wheels/d2/1f/f7/19fbc14b405dad439400cb4761f71c628ea92402b098ed226f\nSuccessfully built pytorch-wavelets\nInstalling collected packages: pytorch-wavelets\nSuccessfully installed pytorch-wavelets-1.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \n[GCC 9.3.0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Write down the plan in words and references. We will then fill in code.","metadata":{"papermill":{"duration":0.007766,"end_time":"2022-04-08T14:26:01.803635","exception":false,"start_time":"2022-04-08T14:26:01.795869","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Encoding: ResNeXt (conv 1-3) => Position Enc. (ref. 2107.14222) => Deep-ViT (w/ EfficientNet)\n# Decoding(for unsupervised training) : HiT(low resolution stage, same # as Deep-ViT) => FCC-GAN\n# Comparison : Transformer-XL \n# Classifier/MLP : MLP head (output 8) => Reconstruction module => F\n\n# Training Steps:\n# 1. Train Encoder (unsupervised): manipulate input image (ref. SiT, + rotation) and match to output\n# 2. Train Comparison (unsupervised) : use different head, mix&match the 2 images (ref. BERT, ALBERT)\n# 3. Train MLP (supervised) : compare output to F\n\n# Inference Steps: Encode each image => concat. 2 images => Comparison => MLP\n\n#offical sample code: https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607","metadata":{"papermill":{"duration":0.014252,"end_time":"2022-04-08T14:26:01.82586","exception":false,"start_time":"2022-04-08T14:26:01.811608","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-05T02:35:39.450273Z","iopub.execute_input":"2022-05-05T02:35:39.450607Z","iopub.status.idle":"2022-05-05T02:35:39.455558Z","shell.execute_reply.started":"2022-05-05T02:35:39.450566Z","shell.execute_reply":"2022-05-05T02:35:39.454671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{"papermill":{"duration":0.007731,"end_time":"2022-04-08T14:26:01.842112","exception":false,"start_time":"2022-04-08T14:26:01.834381","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# copied from sample code\n# Input data files are available in the read-only \"../input/\" directory.\n\n# on kaggle\nsrc = '../input/image-matching-challenge-2022/train'\n\n# on pc\n# src = './image-matching-challenge-2022/train'\n\nval_scenes = []\nfor f in os.scandir(src):\n    if f.is_dir():\n        cur_scene = os.path.split(f)[-1]\n        print(f'Found scene \"{cur_scene}\"\" at {f.path}')\n        val_scenes += [cur_scene]","metadata":{"papermill":{"duration":0.02399,"end_time":"2022-04-08T14:26:01.874438","exception":false,"start_time":"2022-04-08T14:26:01.850448","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-13T03:26:17.496220Z","iopub.execute_input":"2022-05-13T03:26:17.496533Z","iopub.status.idle":"2022-05-13T03:26:17.512274Z","shell.execute_reply.started":"2022-05-13T03:26:17.496501Z","shell.execute_reply":"2022-05-13T03:26:17.511576Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Found scene \"british_museum\"\" at ../input/image-matching-challenge-2022/train/british_museum\nFound scene \"piazza_san_marco\"\" at ../input/image-matching-challenge-2022/train/piazza_san_marco\nFound scene \"trevi_fountain\"\" at ../input/image-matching-challenge-2022/train/trevi_fountain\nFound scene \"st_pauls_cathedral\"\" at ../input/image-matching-challenge-2022/train/st_pauls_cathedral\nFound scene \"colosseum_exterior\"\" at ../input/image-matching-challenge-2022/train/colosseum_exterior\nFound scene \"buckingham_palace\"\" at ../input/image-matching-challenge-2022/train/buckingham_palace\nFound scene \"temple_nara_japan\"\" at ../input/image-matching-challenge-2022/train/temple_nara_japan\nFound scene \"sagrada_familia\"\" at ../input/image-matching-challenge-2022/train/sagrada_familia\nFound scene \"grand_place_brussels\"\" at ../input/image-matching-challenge-2022/train/grand_place_brussels\nFound scene \"pantheon_exterior\"\" at ../input/image-matching-challenge-2022/train/pantheon_exterior\nFound scene \"notre_dame_front_facade\"\" at ../input/image-matching-challenge-2022/train/notre_dame_front_facade\nFound scene \"st_peters_square\"\" at ../input/image-matching-challenge-2022/train/st_peters_square\nFound scene \"sacre_coeur\"\" at ../input/image-matching-challenge-2022/train/sacre_coeur\nFound scene \"taj_mahal\"\" at ../input/image-matching-challenge-2022/train/taj_mahal\nFound scene \"lincoln_memorial_statue\"\" at ../input/image-matching-challenge-2022/train/lincoln_memorial_statue\nFound scene \"brandenburg_gate\"\" at ../input/image-matching-challenge-2022/train/brandenburg_gate\n","output_type":"stream"}]},{"cell_type":"code","source":"# Each scene in the validation set contains a list of images, poses, and pairs. Let's pick one and look at some images.\n'''\nscene = 'piazza_san_marco'\n\nimages_dict = {}\nfor filename in glob(f'{src}/{scene}/images/*.jpg'):\n    cur_id = os.path.basename(os.path.splitext(filename)[0])\n\n    # OpenCV expects BGR, but the images are encoded in standard RGB, so you need to do color conversion if you use OpenCV for I/O.\n    images_dict[cur_id] = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n    \nprint(f'Loaded {len(images_dict)} images.')\n\nnum_rows = 6\nnum_cols = 4\nf, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20), constrained_layout=True)\nfor i, key in enumerate(images_dict):\n    if i >= num_rows * num_cols:\n        break\n    cur_ax = axes[i % num_rows, i // num_rows]\n    cur_ax.imshow(images_dict[key])\n    cur_ax.set_title(key)\n    cur_ax.axis('off')\n'''","metadata":{"papermill":{"duration":7.599256,"end_time":"2022-04-08T14:26:09.482995","exception":false,"start_time":"2022-04-08T14:26:01.883739","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-13T02:42:40.253801Z","iopub.execute_input":"2022-05-13T02:42:40.254056Z","iopub.status.idle":"2022-05-13T02:42:40.262431Z","shell.execute_reply.started":"2022-05-13T02:42:40.25403Z","shell.execute_reply":"2022-05-13T02:42:40.261866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#resize the image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset\nfrom torch.utils.data import Dataset\n\n# this loads 2 imgs at once for the first 2 training steps\nclass IMC_images(Dataset):\n\n    def __init__(self, landmarks, root_dir, transform=lambda x : x):\n        self.landmarks = landmarks\n        self.root_dir = root_dir\n        self.transform = transform\n        \n        self.buffer_data()\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        landmark1 = random.randint(0, len(self.imgs.keys())-1)\n        landmark2 = random.randint(0, len(self.imgs.keys())-1)\n\n        img_num1 = random.randint(0, len(self.imgs[landmark1]))\n        img_num2 = random.randint(0, len(self.imgs[landmark2]))\n\n        img1 = self.imgs[landmark1][img_num1]\n        img2 = self.imgs[landmark2][img_num2]\n\n        comp = []\n        if landmark1 == landmark2 :\n            comp = [1, 0]\n        else :\n            comp = [0, 1]\n        \n        if img_num1 == img_num2 :\n            comp.append(1)\n        else :\n            comp.append(0)\n        \n        sample = {'img1': img1, 'img2': img2, 'comp': comp}\n\n        return sample\n    \n    def buffer_data(self):\n        self.imgs = {}\n        for i, j in enumerate(self.landmarks[:1]):\n            imgs = []\n            for file in glob.glob(self.root_dir + \"/\" + j + '/images/*'):\n                x = self.transform(io.read_image(file, mode=io.ImageReadMode.RGB))\n                imgs.append(x)\n            print(len(imgs))\n            self.imgs[i] = torch.stack((*imgs,), 0)\n                # Length * Height * Width\n    \n\n# dataset\nclass IMC_dataset(Dataset):\n    \"\"\"Image Matching Challenge 2022 dataset\"\"\"\n\n    def __init__(self, cali_file, covis_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.calibration = pd.read_csv(cali_file)\n        self.pair_covisibility = pd.read_csv(covis_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.pair_covisibility)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Returns:\n            image_1 (tensor): First image\n            image_2 (tensor): Secound image\n            covisibility (float)\n            K_1 (matrix): Camera intransic matrix\n            K_2 (matrix): Camera intransic matrix\n            R_1 (matrix): Rotation matrix\n            R_2 (matrix): Rotation matrix\n            T_1 (vector): Translation vector\n            T_2 (vector): Translation vector\n            F (matrix): Fundamental matrix\n        \"\"\"\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        pairID = self.pair_covisibility[\"pair\"][idx]\n\n        img_1_id , img_2_id = pairID.split(\"-\")\n\n        F = self.pair_covisibility[\"fundamental_matrix\"][idx]\n\n        print(img_1_id, img_2_id) \n\n        img_1 = io.read_image(os.path.join(root_dir, img_1_id) + \".jpg\")\n        img_2 = io.read_image(os.path.join(root_dir, img_2_id) + \".jpg\")\n\n        covis = self.pair_covisibility[\"covisibility\"][idx]\n\n        idx_1 = self.calibration.loc[self.calibration['image_id'] == img_1_id]\n        idx_2 = self.calibration.loc[self.calibration['image_id'] == img_2_id]\n\n        K_1 = self.calibration['camera_intrinsics'][idx_1]\n        K_2 = self.calibration['camera_intrinsics'][idx_2]\n\n        R_1 = self.calibration['rotation_matrix'][idx_1]\n        R_2 = self.calibration['rotation_matrix'][idx_2]\n\n        T_1 = self.calibration['translation_vector'][idx_1]\n        T_2 = self.calibration['translation_vector'][idx_2]\n\n        sample = {'img_1': img_1, 'img_2': img_2, 'img_2': img_2, 'img_2': img_2, \\\n         'covisibility': covis, 'K_1': K_1, 'K_2': K_2, 'R_1': R_1, 'R_2': R_2, \\\n         'T_1': T_1, 'T_2': T_2, \"F\": F}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n        \n# cali_file = '.\\\\image-matching-challenge-2022\\\\train\\\\brandenburg_gate\\\\calibration.csv'\n# covis_file = \".\\\\image-matching-challenge-2022\\\\train\\\\brandenburg_gate\\\\pair_covisibility.csv\"\n# root_dir = \".\\\\image-matching-challenge-2022\\\\train\\\\brandenburg_gate\\\\images\"\n\ncali_file = src + '/brandenburg_gate/calibration.csv'\ncovis_file = src + '/brandenburg_gate/pair_covisibility.csv'\nroot_dir = src + '/brandenburg_gate/images'\n\ndataset = IMC_dataset(cali_file, covis_file, root_dir)\n\ndef rot_resize(x):\n    s = x.size()\n    if s[0] > s[1] :\n        return torchvision.transforms.Resize((512, 384))(x)\n    else :\n        return torchvision.transforms.Resize((384, 512))(x)\n\ndef test_imc_img():\n    imc_imgs = IMC_images(\n        val_scenes, src, \n        transform=rot_resize\n    )\n    sample = imc_imgs.__getitem__(0)\n    return sample[\"img1\"]\n","metadata":{"execution":{"iopub.status.busy":"2022-05-13T03:26:25.372571Z","iopub.execute_input":"2022-05-13T03:26:25.372875Z","iopub.status.idle":"2022-05-13T03:26:25.614334Z","shell.execute_reply.started":"2022-05-13T03:26:25.372845Z","shell.execute_reply":"2022-05-13T03:26:25.613433Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# # pytorch data loading\n# def get_scene_trainloader(scene):\n\n#     transform = transforms.Compose(\n#         [transforms.ToTensor(),\n#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n#     trainset = \"./image-matching-challenge-2022/train/\" + scene\n\n#     trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n#                                             shuffle=True, num_workers=2)\n\n#     classes = ('plane', 'car', 'bird', 'cat',\n#             'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n#     def imshow(img):\n#         img = img / 2 + 0.5     # unnormalize\n#         npimg = img.numpy()\n#         plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n\n#     # get some random training images\n#     dataiter = iter(trainloader)\n\n#     print(dataiter.next())\n#     images, labels = dataiter.next()\n\n#     # show images\n#     imshow(torchvision.utils.make_grid(images))\n#     # print labels\n#     print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n\n#     return trainloader\n\n# get_scene_trainloader(\"piazza_san_marco\")","metadata":{"papermill":{"duration":0.098892,"end_time":"2022-04-08T14:26:09.68268","exception":false,"start_time":"2022-04-08T14:26:09.583788","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-13T02:42:55.259877Z","iopub.execute_input":"2022-05-13T02:42:55.260164Z","iopub.status.idle":"2022-05-13T02:42:55.264148Z","shell.execute_reply.started":"2022-05-13T02:42:55.260138Z","shell.execute_reply":"2022-05-13T02:42:55.263574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image manipulation (ref. SiT)\n# paper:\n# github: github.com/Sara-Ahmed/SiT\n# Copy + Pasted from the github \n\ndef drop_rand_patches(X, X_rep=None, max_drop=0.3, max_block_sz=0.25, tolr=0.05):\n    #######################\n    # X_rep: replace X with patches from X_rep. If X_rep is None, replace the patches with Noise\n    # max_drop: percentage of image to be dropped\n    # max_block_sz: percentage of the maximum block to be dropped\n    # tolr: minimum size of the block in terms of percentage of the image size\n    #######################\n    \n    C, H, W = X.size()\n    n_drop_pix = np.random.uniform(0, max_drop)*H*W\n    mx_blk_height = int(H*max_block_sz)\n    mx_blk_width = int(W*max_block_sz)\n    \n    tolr = (int(tolr*H), int(tolr*W))\n    \n    total_pix = 0\n    while total_pix < n_drop_pix:\n        \n        # get a random block by selecting a random row, column, width, height\n        rnd_r = randint(0, H-tolr[0])\n        rnd_c = randint(0, W-tolr[1])\n        rnd_h = min(randint(tolr[0], mx_blk_height)+rnd_r, H) #rnd_r is alread added - this is not height anymore\n        rnd_w = min(randint(tolr[1], mx_blk_width)+rnd_c, W)\n        \n        if X_rep is None:\n            X[:, rnd_r:rnd_h, rnd_c:rnd_w] = torch.empty((C, rnd_h-rnd_r, rnd_w-rnd_c), dtype=X.dtype, device='cpu').normal_()\n        else:\n            X[:, rnd_r:rnd_h, rnd_c:rnd_w] = X_rep[:, rnd_r:rnd_h, rnd_c:rnd_w]    \n         \n        total_pix = total_pix + (rnd_h-rnd_r)*(rnd_w-rnd_c)\n        \n    return X\n\ndef rgb2gray_patch(X, tolr=0.05):\n\n    C, H, W = X.size()\n    tolr = (int(tolr*H), int(tolr*W))\n     \n    # get a random block by selecting a random row, column, width, height\n    rnd_r = randint(0, H-tolr[0])\n    rnd_c = randint(0, W-tolr[1])\n    rnd_h = min(randint(tolr[0], H)+rnd_r, H) #rnd_r is alread added - this is not height anymore\n    rnd_w = min(randint(tolr[1], W)+rnd_c, W)\n    \n    X[:, rnd_r:rnd_h, rnd_c:rnd_w] = torch.mean(X[:, rnd_r:rnd_h, rnd_c:rnd_w], dim=0).unsqueeze(0).repeat(C, 1, 1)\n\n    return X\n    \n\ndef smooth_patch(X, max_kernSz=15, gauss=5, tolr=0.05):\n\n    #get a random kernel size (odd number)\n    kernSz = 2*(randint(3, max_kernSz+1)//2)+1\n    gausFct = np.random.rand()*gauss + 0.1 # generate a real number between 0.1 and gauss+0.1\n    \n    C, H, W = X.size()\n    tolr = (int(tolr*H), int(tolr*W))\n     \n    # get a random block by selecting a random row, column, width, height\n    rnd_r = randint(0, H-tolr[0])\n    rnd_c = randint(0, W-tolr[1])\n    rnd_h = min(randint(tolr[0], H)+rnd_r, H) #rnd_r is alread added - this is not height anymore\n    rnd_w = min(randint(tolr[1], W)+rnd_c, W)\n    \n    \n    gauss = kornia.filters.GaussianBlur2d((kernSz, kernSz), (gausFct, gausFct))\n    X[:, rnd_r:rnd_h, rnd_c:rnd_w] = gauss(X[:, rnd_r:rnd_h, rnd_c:rnd_w].unsqueeze(0))\n    \n    return X\n\ndef random_rotation(X, orientation=None):\n    # generate random number between 0 and n_rot to represent the rotation\n    if orientation is None:\n        orientation = np.random.randint(0, 4)\n    \n    if orientation == 0: # do nothing\n        pass\n    elif orientation == 1:  \n        X = X.rot90(-1, [1, 2])\n    elif orientation == 2:  \n        X = X.rot90(-1, [1, 2]).rot90(-1, [1, 2])\n    elif orientation == 3: \n        X = X.rot90(1, [1, 2])\n    \n    return X, orientation\n\ndef distortImages(samples):\n    n_imgs = samples.size()[0] #this is batch size, but in case bad inistance happened while loading\n    samples_aug = samples.detach().clone().type(torch.float)\n    for i in range(n_imgs):\n        #samples_aug[i] = random_rotation(samples_aug[i])\n        \n        samples_aug[i] = rgb2gray_patch(samples_aug[i])\n        \n        samples_aug[i] = smooth_patch(samples_aug[i])\n\n        samples_aug[i] = drop_rand_patches(samples_aug[i])\n\n        idx_rnd = randint(0, n_imgs)\n        if idx_rnd != i:\n            samples_aug[i] = drop_rand_patches(samples_aug[i], samples_aug[idx_rnd])\n      \n    return samples_aug.type(torch.float16)   \n\ndef test_rand():\n    x = test_imc_img()\n    x = (x[None, :]).type(torch.float)\n    print(x.size())\n    x = distortImages(x)\n    plt.imshow(x[0].permute(1, 2, 0).type(torch.uint8))\n\n#test_rand()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T03:26:29.663173Z","iopub.execute_input":"2022-05-13T03:26:29.663494Z","iopub.status.idle":"2022-05-13T03:26:29.691742Z","shell.execute_reply.started":"2022-05-13T03:26:29.663467Z","shell.execute_reply":"2022-05-13T03:26:29.691030Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# loss \n# !pip install pywavelets\n# !git clone https://github.com/fbcotter/pytorch_wavelets\n# !cd pytorch_wavelets && pip install .\nimport pytorch_wavelets\nfrom pytorch_wavelets import DWTForward, DWTInverse\n\nclass DistillerLoss(nn.Module):\n    def __init__(\n            self,\n            loss_weights={\"l1\": 1.0, \"l2\": 1.0, \"loss_p\": 1.0, \"loss_g\": 0.5}\n    ):\n        super().__init__()\n        # l1/l2 loss\n        self.l1_loss = nn.L1Loss()\n        self.l2_loss = nn.MSELoss()\n        \n        # loss weights\n        self.loss_weights = loss_weights\n        # utils\n        self.dwt = DWTForward(J=1, mode='zero', wave='db1')\n        self.idwt = DWTInverse(mode=\"zero\", wave=\"db1\")\n\n    def forward(self, pred, gt):\n        # l1/l2 loss\n        loss = {\"l1\": 0, \"l2\": 0}\n        for _pred in pred[\"freq\"]:\n            _pred_rgb = self.dwt_to_img(_pred)\n            _gt_rgb = F.interpolate(gt[\"img\"], size=_pred_rgb.size(-1), mode='bilinear', align_corners=True)\n            _gt_freq = self.img_to_dwt(_gt_rgb)\n            loss[\"l1\"] += self.l1_loss(_pred_rgb, _gt_rgb)\n            loss[\"l2\"] += self.l2_loss(_pred_rgb, _gt_rgb)\n            loss[\"l1\"] += self.l1_loss(_pred, _gt_freq)\n            loss[\"l2\"] += self.l2_loss(_pred, _gt_freq)\n            \n        # total loss\n        loss[\"loss\"] = 0\n        for k, w in self.loss_weights.items():\n            if loss[k] is not None:\n                loss[\"loss\"] += w * loss[k]\n            else:\n                del loss[k]\n        return loss[\"loss\"]\n\n    def img_to_dwt(self, img):\n        low, high = self.dwt(img)\n        b, _, _, h, w = high[0].size()\n        high = high[0].view(b, -1, h, w)\n        freq = torch.cat([low, high], dim=1)\n        return freq\n\n    def dwt_to_img(self, img):\n        b, c, h, w = img.size()\n        low = img[:, :3, :, :]\n        high = img[:, 3:, :, :].view(b, 3, 3, h, w)\n        return self.idwt((low, [high]))\n\nclass ComparisonLoss(nn.Module): #work in progress,  but binary cross entropy loss for now\n    def __init__(self):\n        super().__init__()\n        self.bcel = nn.BCELoss()\n    \n    def forward(self, pred, gt):\n        return self.bcel(pred, gt)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T03:26:32.443370Z","iopub.execute_input":"2022-05-13T03:26:32.443957Z","iopub.status.idle":"2022-05-13T03:26:32.724683Z","shell.execute_reply.started":"2022-05-13T03:26:32.443900Z","shell.execute_reply":"2022-05-13T03:26:32.723745Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\n\n# training loop \ndef train_SSL(\n        encoder, decoder, comparison, \n        recon_loss, comp_loss, loss_crit,\n        train_loader, optimizer,\n        device, save_loc,\n        max_norm: float = 0\n    ):\n    model.train(True)\n    print_freq = 50\n\n    loader = pl.MpDeviceLoader(train_loader, device)\n    loss_hist = {\"training loss\": []}\n    #training \n    for i, train_data in enumerate(loader):\n        optimizer.zero_grad()\n\n        with torch.autocast():\n            imgs = torch.cat([train_data[\"img1\"], train_data[\"img2\"]], dim=0)\n            \n            imgs1_aug = distortImages(train_data[\"img1\"]) # Apply distortion\n            imgs2_aug = distortImages(train_data[\"img2\"]) \n            \n            img1_enc = encoder(img1_aug) \n            img1_recon = decoder(img1_enc.detach().clone())\n\n            img2_enc = encoder(img2_aug) \n            img2_recon = decoder(img2_enc.detach().clone())\n\n            imgs_comp = comparison(torch.cat([img1_enc, img2_enc], dim=0))\n\n            imgs_recon = torch.cat([imgs1_recon, imgs2_recon], dim=0) \n            #imgs = torch.cat([imgs1, imgs2], dim=0) \n\n            loss = recon_loss(imgs_recon, imgs) + comp_loss(imgs_comp1, train_data[\"comp\"])\n            loss.backward()\n        \n            xm.optimizer_step(optimizer)\n\n        loss_hist[\"training loss\"] = (loss, i * imgs.size()[0])\n\n        if loss < loss_crit :\n            torch.save(\n                {\n                    \"enc\" : encoder.state_dict(),\n                    \"dec\" : decoder.state_dict(),\n                    \"comp\" : comparison.state_dict(),\n                    \"loss\" : loss.state_dict()\n                },\n                save_loc\n            )\n            \n            break \n\n    return loss_hist\n    #evaluating\n    #commented out because its not very useful\n    '''loader = pl.MpDeviceLoader(test_loader, device)\n\n    for i, img1, img2, same in enumerate(loader):\n        imgs_aug = distortImages(imgs1) # Apply distortion\n\n        with torch.autocast():\n            imgs_recon = model(imgs_aug) \n\n            loss.append(loss(imgs_recon, imgs))\n\n        writer.add_scalar(\"testing loss\", loss, i * imgs.size()[0])'''","metadata":{"execution":{"iopub.status.busy":"2022-05-13T03:27:29.436312Z","iopub.execute_input":"2022-05-13T03:27:29.436644Z","iopub.status.idle":"2022-05-13T03:27:29.451316Z","shell.execute_reply.started":"2022-05-13T03:27:29.436602Z","shell.execute_reply":"2022-05-13T03:27:29.450504Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Summary: A combination of ResNeXt and Squeeze and Excitation Network (SENet) to \n# ResNeXt ref: https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n# SENet ref: https://github.com/moskomule/senet.pytorch \nimport math\n\ndef conv1x1(in_channels, out_channels, stride = 1):\n    \"\"\" 1x1 convolution\"\"\"\n    return nn.Conv2d(\n        in_channels, out_channels,\n        kernel_size = 1, \n        stride = stride,\n        bias = False\n    )\n\ndef conv3x3(in_channels, out_channels, stride = 1, groups = 1, dilation = 1, padding=1):\n    \"\"\" 3x3 convolution \"\"\"\n    return nn.Conv2d(\n        in_channels, out_channels,\n        kernel_size = 3,\n        stride = stride,\n        padding = padding,\n        groups = groups,\n        bias = False,\n        dilation = dilation \n    )\n\ndef conv5x5(in_channels, out_channels, stride = 1, groups = 1, dilation = 1, padding=1):\n    return nn.Conv2d(\n        in_channels, out_channels,\n        kernel_size = 5,\n        stride = stride,\n        padding = padding,\n        groups = groups,\n        bias = False,\n        dilation = dilation \n    )\n\ndef cnn_hw(height, width, stride=1, padding=0, dilation=1, kernel_size=3):\n    out_h = (int(height) + 2 * padding - dilation * (kernel_size - 1) - 1)/stride + 1\n    out_w = (int(width) + 2 * padding - dilation * (kernel_size - 1) - 1)/stride + 1\n    print(f'h, w = {out_h}, {out_w}')\n    if int(out_h) != out_h or int(out_w) != out_w:\n        pad_h = (math.ceil(out_h) - 1) * stride + dilation * (kernel_size - 1) + 1 - int(height)\n        pad_w = (math.ceil(out_w) - 1) * stride + dilation * (kernel_size - 1) + 1 - int(width)\n        return False, (int(pad_h), int(pad_w))\n    return True, (int(out_h), int(out_w))\n\nclass BottleNeck(nn.Module):\n    \"\"\" BottleNeck Layer in ResNet \"\"\"\n\n    expansion : int = 4 \n\n    def __init__(\n        self, in_channels, out_channels,\n        reduction=2, stride=1, dilation=1, padding=1, downsample=None, num_groups=64,\n        conv=conv3x3\n    ):\n        # if we want to use the expansion factor, we can just modify \"out_channels\"\n        # out_channels = in_channels * self.expansion\n\n        super().__init__()\n\n        width = int(in_channels/reduction)\n\n        # inplace is used for ReLU to reduce memory usage\n        self.resnext_block = nn.Sequential(\n            conv1x1(in_channels, width),\n            nn.BatchNorm2d(width),\n            nn.ReLU(inplace=True),\n\n            conv(width, width, padding=padding, stride=stride, dilation=dilation, groups=num_groups),\n            nn.BatchNorm2d(width),\n            nn.ReLU(inplace=True),\n\n            conv1x1(width, out_channels),\n            nn.BatchNorm2d(out_channels)\n        )\n\n        self.downsample = downsample\n\n        self.activation = nn.ReLU(inplace=True)\n            \n    def forward(self, x):\n        residual = x \n        out = self.resnext_block(x)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if out.size() != residual.size():\n            print(f'size mismatch {out.size()} and {residual.size()}')\n        out += residual\n        out = self.activation(out)\n        return out\n\nclass SELayer(nn.Module):\n    \"\"\" building block described in the SENet Paper and github\"\"\"\n    def __init__(self, channel, reduction=16):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel//reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel//reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        batch, channel, _, _ = x.size()\n        y = self.avg_pool(x).view(batch, channel)\n        y = self.fc(y).view(batch, channel, 1, 1)\n        return x * y.expand_as(x)\n\nclass ResNeXtLayer(nn.Module):\n    \"\"\" basic layer described in the ResNeXt Paper \"\"\"\n    def __init__(self, in_channels, out_channels, num_blocks, in_hw, num_groups=64, dilation=1, stride=1):\n        super().__init__()\n        downsample = nn.Sequential(\n            conv1x1(in_channels, out_channels, stride),\n            nn.BatchNorm2d(out_channels)\n        )\n\n        layers = []\n        layers.append(\n            BottleNeck(\n                in_channels, out_channels, \n                downsample=downsample, stride=stride, num_groups=num_groups\n            )\n        )\n        \n        for i in range(1, num_blocks):\n            layers += [ \\\n                BottleNeck(out_channels, out_channels, dilation=dilation, num_groups=num_groups), \\\n                SELayer(out_channels) \\\n            ]\n        \n        hw = cnn_hw(*in_hw, kernel_size=3, stride=stride, dilation=dilation, padding=dilation)\n        self.padding = None\n        if not hw[0] :\n            self.padding = lambda x : torch.nn.functional.pad(x, (hw[1][0], 0, hw[1][1], 0), 'constant', 0)\n            hw = (hw[1][0] + in_hw[0], hw[1][1] + in_hw[1])\n            hw = cnn_hw(*hw, kernel_size=3, stride=stride, dilation=dilation, padding=dilation)\n        self.hw = hw[1]\n        self.layers = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        if self.padding != None :\n            x = self.padding(x)\n        x = self.layers(x)\n        return x\n\ncnn_hw(480, 360, kernel_size=1, stride=2, padding=0)","metadata":{"papermill":{"duration":0.10691,"end_time":"2022-04-08T14:26:09.888627","exception":false,"start_time":"2022-04-08T14:26:09.781717","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-13T03:27:38.907363Z","iopub.execute_input":"2022-05-13T03:27:38.907679Z","iopub.status.idle":"2022-05-13T03:27:38.954145Z","shell.execute_reply.started":"2022-05-13T03:27:38.907650Z","shell.execute_reply":"2022-05-13T03:27:38.953290Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"h, w = 240.5, 180.5\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(False, (1, 1))"},"metadata":{}}]},{"cell_type":"code","source":"# CAPE: Continuous Augmented Positional Embeddings\n# paper @ https://arxiv.org/2106.03143\n\nclass CAPE(nn.Module): # I have offically given up on encoding in general\n    def __init__(\n        self, model_dim, \n        max_global_shift = 0.0, max_local_shift = 0.0, max_global_scaling = 1.0, \n    ):\n        self.max_global_shift = max_global_shift\n        self.max_local_shift = max_local_shift\n        self.max_global_scaling = max_global_scaling\n\n        self.register_buffer('content_scale', nn.Tensor([math.sqrt(model_dim)]))\n\n    def forward(self, patches):\n        return (patches * self.content_scale) + self.compute_pos_emb(patches)\n    \n    def compute_pos_emb(self, patches):\n        batch, height, width, channel = patches.shape()\n\n        x = torch.zeros\n","metadata":{"execution":{"iopub.status.busy":"2022-05-13T03:27:49.895911Z","iopub.execute_input":"2022-05-13T03:27:49.896710Z","iopub.status.idle":"2022-05-13T03:27:49.903428Z","shell.execute_reply.started":"2022-05-13T03:27:49.896673Z","shell.execute_reply":"2022-05-13T03:27:49.902456Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# TransCNN + Universal Transformer \n# Idea : each layer repeated via ACT, then down sampled\n# Idea : direct downsampling for residual\n\nclass CNNAttention(nn.Module):\n    def __init__(self, total_dim, head_dim, conv=conv1x1, grid_size=1, downsample_rate=1, drop=0):\n        super().__init__()\n        self.num_heads = total_dim // head_dim # area of previous step / area of head\n        self.head_dim = head_dim \n        self.side_len = self.head_dim ** -0.5\n        self.grid_size = grid_size\n\n        self.norm = nn.BatchNorm2d(total_dim)\n        self.qkv = conv(total_dim, total_dim * 3)\n        self.proj = conv(total_dim, total_dim)\n        self.drop = nn.Dropout2d(drop, inplace=True)\n\n        if self.grid_size > 1:\n            self.q = conv(total_dim, total_dim)\n            self.kv = conv(total_dim, total_dim * 2)\n\n            self.grid_norm = nn.BatchNorm2d(total_dim)\n            self.avg_pool = nn.AvgPool2d(total_dim)\n            self.downsample_norm = nn.BatchNorm2d(total_dim)\n\n    def forward(self, x):\n        batch, channels, height, width = x.size()\n        qkv = self.qkv(self.norm(x))\n\n        if self.grid_size > 1:\n            # compute grid based/local attention\n            grid_h, grid_w = height // self.grid_size, width // self.grid_size # H/G, W/G\n            qkv = qkv.reshape(\n                batch, 3, # q, k, v\n                self.num_heads, self.head_dim, \n                grid_h, self.grid_size,\n                grid_w, self.grid_size\n            ) # ref. the dimensions of this space is R^ Batch * QKV * Head * Size * H/G * G * W/G * G\n            qkv = qkv.permute(1, 0, 2, 4, 6, 5, 7, 3) # R^ QKV * Batch * Head * H/G * W/G * G * G * Size \n            qkv = qkv.reshape(3, -1, self.grid_size ** 2, self.head_dim) # R^ QKV * (Batch * Head * H/ * G * G * Size \n            q, k, v = qkv[0], qkv[1], qkv[2]\n\n            attn = (q @ k.transpose(-2, -1)) * self.side_len # transpose k -> R^ Batch * G * G * (W * H)\n            attn = attn.softmax(dim=-1)\n            grid_x = (attn @ v).reshape(\n                batch, self.num_heads, \n                grid_h, grid_w, \n                self.grid_size, self.grid_size, \n                self.head_dim \n            ) # R^ Batch * Head * H/G * W/G * G * G * Size, same as after permute\n            grid_x = self.grid_norm(x + grid_x) #residue and normalisation\n\n            # transform qkv for computing global attention\n            q = self.q(grid_x).reshape(batch, self.num_heads, self.head_dim, -1) # R^ Batch * Head * Size * (H * W)\n            q = q.transpose(-2, -1) # R^ Batch * Head * (H * W) * Size \n            kv = self.kv(self.downsample_norm(self.avg_pool(grid_x)))\n            kv = kv.reshape(batch, 2, self.num_heads, self.head_dim, -1) # R^ Batch * KV * Head * Size * (H * W)\n            kv = kv.permute(1, 0, 2, 4, 3) # R^ KV * Batch * Head *  (H * W) * Size \n            k, v = kv[0], kv[1] # R^ Batch * Head * (H * W) * Size\n        else: \n            # transform qkv for computing global attention\n            qkv = qkv.reshape(batch, 3, self.num_heads, self.head_dim, -1) # R^ Batch * QKV * Head * Size * (H * W)\n            qkv = qkv.permute(1, 0, 2, 4, 3) # R^ QKV * Batch * Head * (H * W) * Size\n            q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        # compute global attention\n        attn = (q @ k.transpose(-2, -1)) * self.side_len \n        attn = attn.softmax(dim=1)\n        global_x = (attn @ v).transpose(-2, -1).reshape(batch, channels, height, width)\n\n        # residue\n        if self.grid_size > 1 :\n            global_x += grid_x\n        x = self.drop(self.proj(global_x))\n\n        return x \n\nclass ACT(nn.Module):\n\n    threshold = 1 - 0.1\n\n    def __init__(self, fn, size, max_steps, activation=nn.Sigmoid):\n        super().__init__()\n        self.fn = fn\n        self.activation = activation()\n        self.fc = nn.Linear(size, 1) # What if we replace linear with conv2d?\n\n        self.max_steps = max_steps\n        # !!!be sure to initialise self.p!!!\n    \n    def forward(self, state):\n        # change the shape of input to 3d for ACT\n        shape = state.size()\n        if len(shape) > 3 :\n            state = state.flatten(start_dim=2, end_dim=3)\n        batch, size, _ = state.size()\n\n        halting_probability = torch.zeros(batch, size)\n        remainders = torch.zeros(batch, size)\n        n_updates = torch.zeros(batch, size)\n        previous_state = torch.zeros_like(state)\n\n        def should_continue(h, n, m):\n            return ((h < self.threshold) & (n < m)).byte().any()\n        \n        step = 0\n        while should_continue(halting_probability, n_updates, self.max_steps):\n            # we are avoiding timing signals because we have our own RPE \n            #state = state.flatten(start_dim=0, end_dim=1)\n            #state = self.fc(state).view(batch, size, -1)\n            p = self.activation(self.fc(state.view(batch, size, -1))).squeeze(-1) \n\n            # calculate masks for which ones to halt\n            still_running = (halting_probability < 1.0).float()\n            new_halted = (halting_probability + p * still_running > self.threshold).float() * still_running\n            still_running = (halting_probability + p * still_running <= self.threshold).float() * still_running\n\n            # halt parameters and increment remainders\n            halting_probability += p * still_running\n            remainders += new_halted * (1 - halting_probability)\n            halting_probability += new_halted * remainders \n            n_updates += still_running + new_halted\n            # compute weights to apply to the state and output\n            update_weights = p * still_running + new_halted * remainders \n            state = state.view(shape)\n            print(f'sz{state.size()}')\n            state = self.fn(state)\n            previous_state = \\\n            state.view(batch, size, -1) * update_weights.unsqueeze(-1) + previous_state * (1 - update_weights.unsqueeze(-1))\n            \n            step += 1\n            \n        # change the shape back to the original\n        previous_state = previous_state.view((*shape, ))\n        return previous_state, (remainders, n_updates)\n\nclass TransDownsample(nn.Module):\n    def __init__(self, in_dim, out_dim, activation=nn.SiLU):\n        super().__init__()\n        self.conv = conv3x3(in_dim, out_dim, padding=1, stride=2)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.residual = nn.Conv2d(in_dim, out_dim, 1)\n        self.norm1 = nn.BatchNorm2d(out_dim)\n        self.norm2 = nn.BatchNorm2d(out_dim)\n        self.act = activation(inplace=True)\n\n    def forward(self, x):\n        x1 = self.norm1(self.conv(x))\n        \n        n = torch.ByteTensor(list(x.size()[2:])) % 2\n        if n.any() > 0:\n            x = torch.nn.functional.pad(x, (n[0], 0, n[1], 0), 'replicate')\n            \n        x2 = self.norm2(self.residual(self.pool(x)))\n        print(f'sizes {x.size()}, {x1.size()}, {x2.size()}')\n        x = self.act(x1 + x2)\n        return x\n\nclass TransCNNLayer(nn.Module):\n    def __init__(\n        self, in_dim, out_dim, depth, max_steps, in_hw,\n        head_dim=64, conv=conv1x1, grid_size=1, downsample_rate=1, drop=0,\n        bnconv=3\n    ):\n        super().__init__()\n\n        self.down_sample = TransDownsample(in_dim, out_dim)\n        padding = 0\n        if bnconv is 3 :\n            bnconv = conv3x3\n            padding = 1\n        elif bnconv is 5 :\n            bnconv = conv5x5\n            padding = 2\n            \n        #print(f'h, w = {h}, {w}')\n        hw = cnn_hw(*in_hw, kernel_size=3, stride=2, padding=1)\n        self.padding = None\n        print(f'padding size : {(hw[1][0], 0, hw[1][1], 0)}')\n        if not hw[0] :\n            pad = (hw[1][0], 0, hw[1][1], 0)\n            self.padding = lambda x : torch.nn.functional.pad(x, pad, 'constant', 0)\n            hw = (hw[1][0] + in_hw[0], hw[1][1] + in_hw[1])\n            hw = cnn_hw(*hw, kernel_size=3, stride=2, padding=1)\n        print(f'transcnn layer {hw}')\n        self.hw = hw[1]\n        h, w = self.hw\n        \n        trans_cnn = []\n        for i in range(depth):\n            transcnn_block = nn.Sequential(\n                CNNAttention(\n                    out_dim,\n                    head_dim,\n                    conv=conv,\n                    grid_size=grid_size,\n                    downsample_rate=downsample_rate,\n                    drop=drop\n                ),\n                BottleNeck(out_dim, out_dim, padding=padding, conv=bnconv)\n            )\n            trans_cnn.append(ACT(transcnn_block, h*w, max_steps))\n        self.trans_cnn = nn.ModuleList(trans_cnn)\n        \n    \n    def forward(self, x):\n        print(f'pre padding {x.size()}')\n        if self.padding != None:\n            x = self.padding(x)\n        print(f'post padding {x.size()}')\n        x = self.down_sample(x)\n        for trans_cnn in self.trans_cnn:\n            print(f'trans cnn hw : {x.size()}')\n            x, _ = trans_cnn(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-13T03:27:52.297912Z","iopub.execute_input":"2022-05-13T03:27:52.298222Z","iopub.status.idle":"2022-05-13T03:27:52.347542Z","shell.execute_reply.started":"2022-05-13T03:27:52.298194Z","shell.execute_reply":"2022-05-13T03:27:52.346671Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# MobileStyleGAN w/o the style\n# Q: Can we just ctrl c+p this?\n# A: Yes! Yes we can!\n\nfrom pytorch_wavelets.dwt.lowlevel import *\nimport math\n\ndef _SFB2D(low, highs, g0_row, g1_row, g0_col, g1_col, mode):\n    mode = int_to_mode(mode)\n\n    lh, hl, hh = torch.unbind(highs, dim=2)\n    lo = sfb1d(low, lh, g0_col, g1_col, mode=mode, dim=2)\n    hi = sfb1d(hl, hh, g0_col, g1_col, mode=mode, dim=2)\n    y = sfb1d(lo, hi, g0_row, g1_row, mode=mode, dim=3)\n\n    return y\n\nclass DWTInverse(nn.Module):\n    \"\"\" Performs a 2d DWT Inverse reconstruction of an image\n    Args:\n        wave (str or pywt.Wavelet): Which wavelet to use\n        C: deprecated, will be removed in future\n    \"\"\"\n    def __init__(self, wave='db1', mode='zero', trace_model=False):\n        super().__init__()\n        if isinstance(wave, str):\n            wave = pywt.Wavelet(wave)\n        if isinstance(wave, pywt.Wavelet):\n            g0_col, g1_col = wave.rec_lo, wave.rec_hi\n            g0_row, g1_row = g0_col, g1_col\n        else:\n            if len(wave) == 2:\n                g0_col, g1_col = wave[0], wave[1]\n                g0_row, g1_row = g0_col, g1_col\n            elif len(wave) == 4:\n                g0_col, g1_col = wave[0], wave[1]\n                g0_row, g1_row = wave[2], wave[3]\n        # Prepare the filters\n        filts = prep_filt_sfb2d(g0_col, g1_col, g0_row, g1_row)\n        self.register_buffer('g0_col', filts[0])\n        self.register_buffer('g1_col', filts[1])\n        self.register_buffer('g0_row', filts[2])\n        self.register_buffer('g1_row', filts[3])\n        self.mode = mode\n        self.trace_model = trace_model\n\n    def forward(self, coeffs):\n        yl, yh = coeffs\n        ll = yl\n        mode = mode_to_int(self.mode)\n\n        for h in yh[::-1]:\n            if h is None:\n                h = torch.zeros(ll.shape[0], ll.shape[1], 3, ll.shape[-2],\n                                ll.shape[-1], device=ll.device)\n\n            if ll.shape[-2] > h.shape[-2]:\n                ll = ll[...,:-1,:]\n            if ll.shape[-1] > h.shape[-1]:\n                ll = ll[...,:-1]\n            if not self.trace_model:\n                ll = SFB2D.apply(ll, h, self.g0_col, self.g1_col, self.g0_row, self.g1_row, mode)\n            else:\n                ll = _SFB2D(ll, h, self.g0_col, self.g1_col, self.g0_row, self.g1_row, mode)\n        return ll\n\nclass IDWTUpsample(nn.Module):\n    def __init__(\n            self,\n            channels_in,\n    ):\n        super().__init__()\n        self.channels = channels_in // 4\n        assert self.channels * 4 == channels_in\n        # upsample\n        self.idwt = DWTInverse(mode='zero', wave='db1')\n\n    def forward(self, x):\n        b, _, h, w = x.size()\n        low = x[:, :self.channels]\n        high = x[:, self.channels:]\n        high = high.view(b, self.channels, 3, h, w)\n        x = self.idwt((low, [high]))\n        return x\n\nclass ModulatedConv2d(nn.Module):\n    def __init__(\n            self,\n            channels_in,\n            channels_out,\n            kernel_size=1\n    ):\n        super().__init__()\n        # create conv\n        self.weight = nn.Parameter(\n            torch.randn(channels_out, channels_in, kernel_size, kernel_size)\n        )\n        # some service staff\n        self.scale = 1.0 / math.sqrt(channels_in * kernel_size ** 2)\n        self.padding = kernel_size // 2\n\n    def forward(self, x):\n        x = F.conv2d(x, self.weight, padding=self.padding)\n        return x\n\n\nclass ModulatedDWConv2d(nn.Module):\n    def __init__( \n            self, \n            channels_in, \n            channels_out, \n            kernel_size=1 \n    ):\n        super().__init__()\n        # create conv\n        self.weight_dw = nn.Parameter(\n            torch.randn(channels_in, 1, int(kernel_size), int(kernel_size))\n        )\n        self.weight_permute = nn.Parameter(\n            torch.randn((channels_out, channels_in, 1, 1))\n        )\n        # some service staff\n        self.scale = 1.0 / math.sqrt(channels_in * kernel_size ** 2)\n        self.padding = kernel_size // 2\n\n    def forward(self, x):\n        x = F.conv2d(x, self.weight_dw, padding=self.padding, groups=x.size(1))\n        x = F.conv2d(x, self.weight_permute)\n        return x\n\nclass StyledConv2d(nn.Module):\n    def __init__(\n        self,\n        channels_in,\n        channels_out,\n        conv_module=ModulatedDWConv2d,\n        kernel_size=1\n    ):\n        super().__init__()\n\n        self.conv = ModulatedDWConv2d(\n            channels_in,\n            channels_out,\n            kernel_size=kernel_size\n        )\n\n        self.bias = nn.Parameter(torch.zeros(1, channels_out, 1, 1))\n        self.act = nn.LeakyReLU(0.2)\n\n    def forward(self, input):\n        out = self.conv(input)\n        out = self.act(out + self.bias)\n        return out\n\nclass MultichannelImage(nn.Module):\n    def __init__(\n            self,\n            channels_in,\n            channels_out,\n            kernel_size=1\n    ):\n        super().__init__()\n        self.conv = ModulatedConv2d(channels_in, channels_out, kernel_size)\n        self.bias = nn.Parameter(torch.zeros(1, channels_out, 1, 1))\n\n    def forward(self, hidden):\n        out = self.conv(hidden)\n        out = out + self.bias\n        return out\n\nclass MobileSynthesisBlock(nn.Module):\n    def __init__(\n            self, \n            channels_in, \n            channels_out, \n            conv_module=ModulatedDWConv2d,\n            kernel_size : int = 1 \n    ):\n        super().__init__()\n        self.up = IDWTUpsample(channels_in)\n        self.conv1 = StyledConv2d(\n            channels_in // 4,\n            channels_out,\n            conv_module=conv_module,\n            kernel_size=kernel_size\n        )\n        self.conv2 = StyledConv2d(\n            channels_out,\n            channels_out,\n            conv_module=conv_module,\n            kernel_size=kernel_size\n        )\n        self.to_img = MultichannelImage(\n            channels_in=channels_out,\n            channels_out=12,\n            kernel_size=1\n        )\n\n    def forward(self, hidden):\n        hidden = self.up(hidden)\n        hidden = self.conv1(hidden)\n        hidden = self.conv2(hidden)\n        img = self.to_img(hidden)\n        return hidden, img\n\n    def wsize(self):\n        return 3","metadata":{"execution":{"iopub.status.busy":"2022-05-13T03:27:56.960471Z","iopub.execute_input":"2022-05-13T03:27:56.960827Z","iopub.status.idle":"2022-05-13T03:27:57.001679Z","shell.execute_reply.started":"2022-05-13T03:27:56.960792Z","shell.execute_reply":"2022-05-13T03:27:57.000787Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, in_hw, head_dim=64, num_channels=3):\n        # conv7x7 + ResNet [1-3] + TransCNN [4-5]\n        super().__init__()\n        self.conv1 = nn.Conv2d(num_channels, 64, 7, stride=2)\n        self.pool1 = nn.MaxPool2d(3, stride=2)\n        \n        hw1 = cnn_hw(*in_hw, kernel_size=7, stride=2)\n        if not hw1[0] :\n            pad = (hw1[1][0], 0, hw1[1][1], 0)\n            self.pad1 = lambda x : torch.nn.functional.pad(x, pad, 'constant', 0)\n            hw1 = (hw1[1][0] + in_hw[0], hw1[1][1] + in_hw[1])\n            hw1 = cnn_hw(*hw1, kernel_size=7, stride=2)\n            \n        hw2 = cnn_hw(*hw1[1], kernel_size=3, stride=2)\n        if not hw2[0] :\n            pad = (hw2[1][0], 0, hw2[1][1], 0)\n            self.pad2 = lambda x : torch.nn.functional.pad(x, pad, 'constant', 0)\n            hw2 = (hw2[1][0] + hw1[1][0], hw2[1][1] + hw1[1][1])\n            hw2 = cnn_hw(*hw2, kernel_size=3, stride=2)\n        print(\"resnext\")\n        self.resnext1 = ResNeXtLayer(64, 256, 3, hw2[1], num_groups=32, stride=1)\n        self.resnext2 = ResNeXtLayer(256, 512, 4, self.resnext1.hw, num_groups=32, stride=1)\n        print(\"transcnn\")\n        self.transcnn1 = TransCNNLayer(512, 512, 1, 1, self.resnext2.hw, bnconv=5, head_dim=head_dim)\n        self.transcnn2 = TransCNNLayer(512, 512, 1, 1, self.transcnn1.hw, bnconv=3, head_dim=head_dim)\n    \n    def forward(self, x):\n        x = self.pad1(x)\n        x = self.conv1(x)\n        x = self.pad2(x)\n        x = self.pool1(x)\n        \n        print(x.size())\n        x = self.resnext1(x)\n        print(x.size())\n        x = self.resnext2(x)\n        print(x.size())\n        x = self.transcnn1(x)\n        print(x.size())\n        x = self.transcnn2(x)\n        \n        return x\n\nclass Decoder(nn.Module):\n    def __init__(\n            self, in_hw, head_dim=64, conv=conv1x1, grid_size=1,\n            trans_channels = [512, 512], max_steps = 4,\n            cnn_channels = [512, 256, 128, 64]\n    ):\n        super().__init__()\n        h, w = in_hw\n        # We need to add \n        # Backwards version of TransCNN\n        self.trans_layers = nn.ModuleList()\n        for i, channels_out in enumerate(trans_channels):\n            transcnn_block = CNNAttention(\n                channels_out,\n                head_dim,\n                conv=conv,\n                grid_size=grid_size,\n                downsample_rate=1,\n                drop=0\n            )\n            self.trans_layers.append(ACT(transcnn_block, h*w, max_steps))\n\n        self.cnn_layers = nn.ModuleList()\n        channels_in = cnn_channels[0]\n        for i, channels_out in enumerate(cnn_channels[1:]):\n            self.cnn_layers.append(\n                MobileSynthesisBlock(\n                    channels_in,\n                    channels_out,\n                    conv_module=ModulatedDWConv2d,\n                    kernel_size=1\n                )\n            )\n            channels_in = channels_out\n\n        self.idwt = DWTInverse(mode=\"zero\", wave=\"db1\")\n        self.register_buffer(\"device_info\", torch.zeros(1))\n        self.trace_model = False\n\n    def forward(self, x):\n        out = {\"freq\": [], \"img\": None}\n\n        for trans_cnn in self.trans_layers:\n            x, _ = trans_cnn(x)\n        \n        for i, m in enumerate(self.cnn_layers):\n            x, freq = m(x)\n            out[\"freq\"].append(freq)\n\n        out[\"img\"] = rot_resize(self.dwt_to_img(out[\"freq\"][-1]))\n        return out\n\n    def dwt_to_img(self, img):\n        b, c, h, w = img.size()\n        low = img[:, :3, :, :]\n        high = img[:, 3:, :, :].view(b, 3, 3, h, w)\n        return self.idwt((low, [high]))\n\n    def wsize(self):\n        return len(self.cnn_layers) * self.cnn_layers[0].wsize() + 2\n\nclass CompHead(nn.Module):\n    def __init__(self, in_size, comp_size):\n        super().__init__()\n        self.seq = nn.Sequential(\n            nn.Linear(in_size, int(in_size ** 0.5)),\n            nn.Linear(int(in_size ** 0.5), comp_size)\n        )\n        self.softmax = nn.Softmax(dim=0)\n    \n    def forward(self, x):\n        x = x.flatten(start_dim=2, end_dim=3)\n        x = self.seq(x)\n        x = self.softmax(x)\n        return x\n    \ndef test_enc_dec():\n    enc = Encoder((512, 384))\n    dec = Decoder((26, 34))\n    comp = CompHead(26*34, 2)\n    x = test_imc_img()\n    x = (x[None, :]).type(torch.float16)\n    print(x.size())\n    x = distortImages(x)\n    \n    #x = enc(x)    \n    #print(x.size())\n    #y = comp(x)\n    #print(y.size())\n    #x = dec(x)\n    #print(x['img'].size())\ntest_enc_dec()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T03:28:00.708052Z","iopub.execute_input":"2022-05-13T03:28:00.708345Z","iopub.status.idle":"2022-05-13T03:28:06.648436Z","shell.execute_reply.started":"2022-05-13T03:28:00.708318Z","shell.execute_reply":"2022-05-13T03:28:06.647421Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"h, w = 253.5, 189.5\nh, w = 254.0, 190.0\nh, w = 126.5, 94.5\nh, w = 127.0, 95.0\nresnext\nh, w = 127.0, 95.0\nh, w = 127.0, 95.0\ntranscnn\nh, w = 64.0, 48.0\npadding size : (64, 0, 48, 0)\ntranscnn layer (True, (64, 48))\nh, w = 32.5, 24.5\npadding size : (3, 0, 3, 0)\nh, w = 34.0, 26.0\ntranscnn layer (True, (34, 26))\n176\ntorch.Size([1, 3, 384, 512])\n","output_type":"stream"}]},{"cell_type":"code","source":"save_loc = './'\n\ndef main():\n    \n    enc = Encoder((512, 384))\n    dec = Decoder((26, 34))\n    comp = CompHead(26*34, 2)\n    \n    recon_loss = DistillerLoss()\n    comp_loss = ComparisonLoss()\n    loss_crit = 0.5\n    \n    \n    imc_imgs = IMC_images(\n        val_scenes, src, \n        transform=rot_resize\n    )\n    device = xm.xla_device()\n    imgs_loader = DataLoader(imc_imgs, batch_size=16,)\n    train_loader = pl.MpDeviceLoader(imc_imgs, device)\n    \n    optim = torch.optim.AdamW(\n        ([enc.parameters(), dec.parameters(), comp.parameters()]),\n        lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True,\n    ) \n    \n    train_SSL(\n        enc, dec, comp, \n        recon_loss, comp_loss, loss_crit,\n        train_loader, optimizer,\n        device, save_loc\n    )\n    \n    \nmain()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T03:30:50.250888Z","iopub.execute_input":"2022-05-13T03:30:50.251874Z","iopub.status.idle":"2022-05-13T03:30:53.944964Z","shell.execute_reply.started":"2022-05-13T03:30:50.251834Z","shell.execute_reply":"2022-05-13T03:30:53.944199Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"h, w = 253.5, 189.5\nh, w = 254.0, 190.0\nh, w = 126.5, 94.5\nh, w = 127.0, 95.0\nresnext\nh, w = 127.0, 95.0\nh, w = 127.0, 95.0\ntranscnn\nh, w = 64.0, 48.0\npadding size : (64, 0, 48, 0)\ntranscnn layer (True, (64, 48))\nh, w = 32.5, 24.5\npadding size : (3, 0, 3, 0)\nh, w = 34.0, 26.0\ntranscnn layer (True, (34, 26))\n176\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_42/3255734708.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_42/3255734708.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     optim = torch.optim.AdamW(\n\u001b[1;32m     23\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     ) \n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     45\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     46\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36madd_param_group\u001b[0;34m(self, param_group)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 raise TypeError(\"optimizer can only optimize Tensors, \"\n\u001b[0;32m--> 255\u001b[0;31m                                 \"but one of the params is \" + torch.typename(param))\n\u001b[0m\u001b[1;32m    256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can't optimize a non-leaf Tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: optimizer can only optimize Tensors, but one of the params is Module.parameters"],"ename":"TypeError","evalue":"optimizer can only optimize Tensors, but one of the params is Module.parameters","output_type":"error"}]}]}