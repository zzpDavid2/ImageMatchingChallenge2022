{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport numpy as np\nimport cv2\nimport csv\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple\nfrom copy import deepcopy\nfrom tqdm import tqdm\nimport random\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\n\n# Check that you're using a recent OpenCV version.\nassert cv2.__version__ > '4.5', 'Please use OpenCV 4.5 or later.'","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.405206,"end_time":"2022-04-08T14:26:01.787265","exception":false,"start_time":"2022-04-08T14:26:01.382059","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-09T13:42:23.039666Z","iopub.execute_input":"2022-04-09T13:42:23.040019Z","iopub.status.idle":"2022-04-09T13:42:23.048228Z","shell.execute_reply.started":"2022-04-09T13:42:23.039983Z","shell.execute_reply":"2022-04-09T13:42:23.047033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write down the plan in words and references. We will then fill in code.","metadata":{"papermill":{"duration":0.007766,"end_time":"2022-04-08T14:26:01.803635","exception":false,"start_time":"2022-04-08T14:26:01.795869","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Encoding: ResNeXt (conv 1-3) => Position Enc. (ref. 2107.14222) => Deep-ViT (w/ EfficientNet)\n# Decoding(for unsupervised training) : HiT(low resolution stage, same # as Deep-ViT) => FCC-GAN\n# Comparison : Transformer-XL \n# Classifier/MLP : MLP head (output 8) => Reconstruction module => F\n\n# Training Steps:\n# 1. Train Encoder (unsupervised): manipulate input image (ref. SiT, + rotation) and match to output\n# 2. Train Comparison (unsupervised) : use different head, mix&match the 2 images (ref. BERT, ALBERT)\n# 3. Train MLP (supervised) : compare output to F\n\n# Inference Steps: Encode each image => concat. 2 images => Comparison => MLP\n\n#offical sample code: https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607","metadata":{"execution":{"iopub.execute_input":"2022-04-08T14:26:01.824321Z","iopub.status.busy":"2022-04-08T14:26:01.823683Z","iopub.status.idle":"2022-04-08T14:26:01.82526Z","shell.execute_reply":"2022-04-08T14:26:01.825701Z"},"papermill":{"duration":0.014252,"end_time":"2022-04-08T14:26:01.82586","exception":false,"start_time":"2022-04-08T14:26:01.811608","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{"papermill":{"duration":0.007731,"end_time":"2022-04-08T14:26:01.842112","exception":false,"start_time":"2022-04-08T14:26:01.834381","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# copied from sample code\n# Input data files are available in the read-only \"../input/\" directory.\n\n# on kaggle\nsrc = '../input/image-matching-challenge-2022/train'\n\n# on pc\n# src = './image-matching-challenge-2022/train'\n\nval_scenes = []\nfor f in os.scandir(src):\n    if f.is_dir():\n        cur_scene = os.path.split(f)[-1]\n        print(f'Found scene \"{cur_scene}\"\" at {f.path}')\n        val_scenes += [cur_scene]","metadata":{"papermill":{"duration":0.02399,"end_time":"2022-04-08T14:26:01.874438","exception":false,"start_time":"2022-04-08T14:26:01.850448","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-09T13:43:17.302548Z","iopub.execute_input":"2022-04-09T13:43:17.302881Z","iopub.status.idle":"2022-04-09T13:43:17.319887Z","shell.execute_reply.started":"2022-04-09T13:43:17.302849Z","shell.execute_reply":"2022-04-09T13:43:17.319024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Each scene in the validation set contains a list of images, poses, and pairs. Let's pick one and look at some images.\n\nscene = 'piazza_san_marco'\n\nimages_dict = {}\nfor filename in glob(f'{src}/{scene}/images/*.jpg'):\n    cur_id = os.path.basename(os.path.splitext(filename)[0])\n\n    # OpenCV expects BGR, but the images are encoded in standard RGB, so you need to do color conversion if you use OpenCV for I/O.\n    images_dict[cur_id] = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n    \nprint(f'Loaded {len(images_dict)} images.')\n\nnum_rows = 6\nnum_cols = 4\nf, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20), constrained_layout=True)\nfor i, key in enumerate(images_dict):\n    if i >= num_rows * num_cols:\n        break\n    cur_ax = axes[i % num_rows, i // num_rows]\n    cur_ax.imshow(images_dict[key])\n    cur_ax.set_title(key)\n    cur_ax.axis('off')","metadata":{"papermill":{"duration":7.599256,"end_time":"2022-04-08T14:26:09.482995","exception":false,"start_time":"2022-04-08T14:26:01.883739","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-09T13:43:21.127891Z","iopub.execute_input":"2022-04-09T13:43:21.128167Z","iopub.status.idle":"2022-04-09T13:43:28.798108Z","shell.execute_reply.started":"2022-04-09T13:43:21.128138Z","shell.execute_reply":"2022-04-09T13:43:28.797216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset\nclass IMC_dataset(Dataset):\n    \"\"\"Image Matching Challenge 2022 dataset\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.landmarks_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.root_dir,\n                                self.landmarks_frame.iloc[idx, 0])\n        image = io.imread(img_name)\n        landmarks = self.landmarks_frame.iloc[idx, 1:]\n        landmarks = np.array([landmarks])\n        landmarks = landmarks.astype('float').reshape(-1, 2)\n        sample = {'image': image, 'landmarks': landmarks}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-04-09T14:32:54.022813Z","iopub.status.idle":"2022-04-09T14:32:54.023145Z","shell.execute_reply.started":"2022-04-09T14:32:54.022977Z","shell.execute_reply":"2022-04-09T14:32:54.022993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pytorch data loading\n\ndef get_scene_trainloader(scene):\n\n    transform = transforms.Compose(\n        [transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n    trainset = \"./image-matching-challenge-2022/train/\" + scene\n\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                            shuffle=True, num_workers=2)\n\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    classes = ('plane', 'car', 'bird', 'cat',\n            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n    def imshow(img):\n        img = img / 2 + 0.5     # unnormalize\n        npimg = img.numpy()\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n\n    # get some random training images\n    dataiter = iter(trainloader)\n\n    print(dataiter.next())\n    images, labels = dataiter.next()\n\n    # show images\n    imshow(torchvision.utils.make_grid(images))\n    # print labels\n    print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n\n    return trainloader\n\nget_scene_trainloader(\"piazza_san_marco\")","metadata":{"papermill":{"duration":0.098892,"end_time":"2022-04-08T14:26:09.68268","exception":false,"start_time":"2022-04-08T14:26:09.583788","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-09T14:32:59.475138Z","iopub.execute_input":"2022-04-09T14:32:59.475677Z","iopub.status.idle":"2022-04-09T14:32:59.51054Z","shell.execute_reply.started":"2022-04-09T14:32:59.475646Z","shell.execute_reply":"2022-04-09T14:32:59.509448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ResNeXt\n# github: https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py\n# doc: https://pytorch.org/hub/pytorch_vision_resnext/\n","metadata":{"execution":{"iopub.execute_input":"2022-04-08T14:26:09.885364Z","iopub.status.busy":"2022-04-08T14:26:09.884364Z","iopub.status.idle":"2022-04-08T14:26:09.8879Z","shell.execute_reply":"2022-04-08T14:26:09.888436Z"},"papermill":{"duration":0.10691,"end_time":"2022-04-08T14:26:09.888627","exception":false,"start_time":"2022-04-08T14:26:09.781717","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}